{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Data Generation SDK\n",
    "\n",
    "We are going to generate a dataset of squat videos with instructions how to perform them, so that we can train an AI pesonal trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from datagen import DatagenConfig\n",
    "\n",
    "config_params = {\n",
    "    'openai': {\n",
    "        'type': 'azure', # openai/azure\n",
    "        'temperature': '1',\n",
    "        'deployment': 'gpt4o' # model for openai / deployment for azure\n",
    "    },\n",
    "    'data_dir': './tmp/squats'\n",
    "}\n",
    "\n",
    "!mkdir -p {config_params['data_dir']}\n",
    "\n",
    "# this config handles all the bookeeping so you need to pass it everywhere.\n",
    "config = DatagenConfig(**config_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a list of search queries to search for videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how to do squats instructional video', 'proper squat technique tutorial']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datagen import get_queries\n",
    "queries = get_queries(\n",
    "    config=config,\n",
    "    prompt='I want to find instructional videos about how to do squats.',\n",
    "    num_queries=2\n",
    ")\n",
    "queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download video information for each query.\n",
    "\n",
    "We'll get 2 videos for each query.<br>\n",
    "One video might be found with multiple queries, so we might get less than `n_queries*videos_per_query` videos.<br>\n",
    "If you want to get all youtube videos for a query, don't pass `videos_per_query` parameter.\n",
    "\n",
    "You can limit the search to only videos licensed with Creative Commons (as indicated by youtube).<br>\n",
    "As this search isn't directly implemented in searching libraries yet, we search for all videos and filter for license afterwards.<br>\n",
    "Unfortunately, this way you will likely get very few results, so use with caution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.13s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['gcNh17Ckjgg', 'EbOPpWi4L8s', 'bEv6CCg2BC8', 'xqvCmoLULNY']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datagen import get_video_ids\n",
    "ids = get_video_ids(queries, config=config, videos_per_query=2,only_creative_commons=False)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(config.data_dir / 'video_ids.json') as f:\n",
    "    ids = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download videos and autogenerated subtitles\n",
    "\n",
    "You can change sub languages, formats etc with `yt_dlp_opts` dictionary (refer to https://github.com/yt-dlp/yt-dlp).<br>\n",
    "The SDK is expecting `.mp4` video files (for now), so don't change that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=gcNh17Ckjgg\n",
      "[youtube] gcNh17Ckjgg: Downloading webpage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] gcNh17Ckjgg: Downloading ios player API JSON\n",
      "[youtube] gcNh17Ckjgg: Downloading tv player API JSON\n",
      "[youtube] gcNh17Ckjgg: Downloading player bd3293c9\n",
      "[youtube] gcNh17Ckjgg: Downloading m3u8 information\n",
      "[info] gcNh17Ckjgg: Downloading subtitles: en\n",
      "[info] gcNh17Ckjgg: Downloading 1 format(s): 18\n",
      "[info] Writing video subtitles to: tmp/squats/videos/gcNh17Ckjgg.en.vtt\n",
      "[download] Destination: tmp/squats/videos/gcNh17Ckjgg.en.vtt\n",
      "[download] 100% of   64.83KiB in 00:00:00 at 1.25MiB/s\n",
      "[download] Destination: tmp/squats/videos/gcNh17Ckjgg.mp4\n",
      "[download] 100% of   14.15MiB in 00:00:00 at 27.23MiB/s    \n",
      "[MoveFiles] Moving file \"tmp/squats/videos/gcNh17Ckjgg.en.vtt\" to \"tmp/squats/subs/gcNh17Ckjgg.en.vtt\"\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=EbOPpWi4L8s\n",
      "[youtube] EbOPpWi4L8s: Downloading webpage\n",
      "[youtube] EbOPpWi4L8s: Downloading ios player API JSON\n",
      "[youtube] EbOPpWi4L8s: Downloading tv player API JSON\n",
      "[youtube] EbOPpWi4L8s: Downloading m3u8 information\n",
      "[info] EbOPpWi4L8s: Downloading subtitles: en\n",
      "[info] EbOPpWi4L8s: Downloading 1 format(s): 18\n",
      "[info] Writing video subtitles to: tmp/squats/videos/EbOPpWi4L8s.en.vtt\n",
      "[download] Destination: tmp/squats/videos/EbOPpWi4L8s.en.vtt\n",
      "[download] 100% of   12.98KiB in 00:00:00 at 386.02KiB/s\n",
      "[download] Destination: tmp/squats/videos/EbOPpWi4L8s.mp4\n",
      "[download] 100% of    4.81MiB in 00:00:00 at 32.36MiB/s    \n",
      "[MoveFiles] Moving file \"tmp/squats/videos/EbOPpWi4L8s.en.vtt\" to \"tmp/squats/subs/EbOPpWi4L8s.en.vtt\"\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=bEv6CCg2BC8\n",
      "[youtube] bEv6CCg2BC8: Downloading webpage\n",
      "[youtube] bEv6CCg2BC8: Downloading ios player API JSON\n",
      "[youtube] bEv6CCg2BC8: Downloading tv player API JSON\n",
      "[youtube] bEv6CCg2BC8: Downloading m3u8 information\n",
      "[info] bEv6CCg2BC8: Downloading subtitles: en\n",
      "[info] bEv6CCg2BC8: Downloading 1 format(s): 18\n",
      "[info] Writing video subtitles to: tmp/squats/videos/bEv6CCg2BC8.en.vtt\n",
      "[download] Destination: tmp/squats/videos/bEv6CCg2BC8.en.vtt\n",
      "[download] 100% of  111.79KiB in 00:00:00 at 1.97MiB/s\n",
      "[download] Destination: tmp/squats/videos/bEv6CCg2BC8.mp4\n",
      "[download] 100% of   21.82MiB in 00:00:00 at 25.69MiB/s    \n",
      "[MoveFiles] Moving file \"tmp/squats/videos/bEv6CCg2BC8.en.vtt\" to \"tmp/squats/subs/bEv6CCg2BC8.en.vtt\"\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=xqvCmoLULNY\n",
      "[youtube] xqvCmoLULNY: Downloading webpage\n",
      "[youtube] xqvCmoLULNY: Downloading ios player API JSON\n",
      "[youtube] xqvCmoLULNY: Downloading tv player API JSON\n",
      "[youtube] xqvCmoLULNY: Downloading m3u8 information\n",
      "[info] xqvCmoLULNY: Downloading subtitles: en\n",
      "[info] xqvCmoLULNY: Downloading 1 format(s): 18\n",
      "[info] Writing video subtitles to: tmp/squats/videos/xqvCmoLULNY.en.vtt\n",
      "[download] Destination: tmp/squats/videos/xqvCmoLULNY.en.vtt\n",
      "[download] 100% of    6.00KiB in 00:00:00 at 209.18KiB/s\n",
      "[download] Destination: tmp/squats/videos/xqvCmoLULNY.mp4\n",
      "[download] 100% of    1.11MiB in 00:00:00 at 2.45MiB/s     \n",
      "[MoveFiles] Moving file \"tmp/squats/videos/xqvCmoLULNY.en.vtt\" to \"tmp/squats/subs/xqvCmoLULNY.en.vtt\"\n"
     ]
    }
   ],
   "source": [
    "from datagen import download_videos\n",
    "download_videos(ids, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect segments from video\n",
    "\n",
    "We will use the clip version because it's much faster than gpt4o, but we'll need a GPU.\n",
    "You can also try using CPU for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModel\n",
    "\n",
    "# remove .cuda() for cpu\n",
    "# SIGLIP outputs independent probs as opposed to CLIP that outputs multiclass probs\n",
    "model = AutoModel.from_pretrained(\"google/siglip-so400m-patch14-384\").cuda()\n",
    "processor = AutoProcessor.from_pretrained(\"google/siglip-so400m-patch14-384\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-05 20:01:11.233330 EbOPpWi4L8s - starting - 193 frames\n",
      "2024-08-05 20:01:13.141179 EbOPpWi4L8s - batch 0 - starting\n",
      "2024-08-05 20:01:32.829624 EbOPpWi4L8s - batch 1 - starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:39<01:59, 39.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-05 20:01:51.128046 EbOPpWi4L8s - all batches done - detecting segments\n",
      "2024-08-05 20:01:51.215352 gcNh17Ckjgg - starting - 870 frames\n",
      "2024-08-05 20:02:02.076371 gcNh17Ckjgg - batch 0 - starting\n",
      "2024-08-05 20:02:20.961139 gcNh17Ckjgg - batch 1 - starting\n",
      "2024-08-05 20:02:39.755510 gcNh17Ckjgg - batch 2 - starting\n",
      "2024-08-05 20:02:58.627160 gcNh17Ckjgg - batch 3 - starting\n",
      "2024-08-05 20:03:17.502791 gcNh17Ckjgg - batch 4 - starting\n",
      "2024-08-05 20:03:36.482962 gcNh17Ckjgg - batch 5 - starting\n",
      "2024-08-05 20:03:55.420918 gcNh17Ckjgg - batch 6 - starting\n",
      "2024-08-05 20:04:14.296455 gcNh17Ckjgg - batch 7 - starting\n",
      "2024-08-05 20:04:33.144871 gcNh17Ckjgg - batch 8 - starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [03:35<03:59, 119.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-05 20:04:46.854691 gcNh17Ckjgg - all batches done - detecting segments\n",
      "2024-08-05 20:04:46.951287 bEv6CCg2BC8 - starting - 1318 frames\n",
      "2024-08-05 20:04:59.902384 bEv6CCg2BC8 - batch 0 - starting\n",
      "2024-08-05 20:05:18.849763 bEv6CCg2BC8 - batch 1 - starting\n",
      "2024-08-05 20:05:37.729374 bEv6CCg2BC8 - batch 2 - starting\n",
      "2024-08-05 20:05:56.567156 bEv6CCg2BC8 - batch 3 - starting\n",
      "2024-08-05 20:06:15.475718 bEv6CCg2BC8 - batch 4 - starting\n",
      "2024-08-05 20:06:34.447244 bEv6CCg2BC8 - batch 5 - starting\n",
      "2024-08-05 20:06:53.330642 bEv6CCg2BC8 - batch 6 - starting\n",
      "2024-08-05 20:07:12.193763 bEv6CCg2BC8 - batch 7 - starting\n",
      "2024-08-05 20:07:31.124894 bEv6CCg2BC8 - batch 8 - starting\n",
      "2024-08-05 20:07:50.040327 bEv6CCg2BC8 - batch 9 - starting\n",
      "2024-08-05 20:08:08.981399 bEv6CCg2BC8 - batch 10 - starting\n",
      "2024-08-05 20:08:27.879922 bEv6CCg2BC8 - batch 11 - starting\n",
      "2024-08-05 20:08:46.790233 bEv6CCg2BC8 - batch 12 - starting\n",
      "2024-08-05 20:09:05.719565 bEv6CCg2BC8 - batch 13 - starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [07:58<03:05, 185.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-05 20:09:09.380070 bEv6CCg2BC8 - all batches done - detecting segments\n",
      "2024-08-05 20:09:09.509757 xqvCmoLULNY - starting - 97 frames\n",
      "2024-08-05 20:09:10.448225 xqvCmoLULNY - batch 0 - starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [08:17<00:00, 124.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-05 20:09:28.786903 xqvCmoLULNY - all batches done - detecting segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datagen import detect_segments_clip\n",
    "\n",
    "from typing import Optional\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "detect_segments_clip(\n",
    "    # video_ids=['KvRK5Owqzgw'],\n",
    "    text_prompts='a person doing squats', # that's the text for CLIP to compare to images. You can provide a list of texts to use average distance.\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    fps_sampling=2, # the more fps, the more granular segment borders and more precise segments, at the cost of speed.\n",
    "    device='cuda', # 'cpu' for local\n",
    "    frames_per_batch=100, # 100 frames use about 10GB GPU RAM, so batch to fill your GPU RAM.\n",
    "    config=config,\n",
    "\n",
    "    # Parameters for segment detection from probabilities - these default values should work well, but if they produce bad results for specific kinds of videos, you can adjust them.\n",
    "    min_prob=0.1, # minimum CLIP probability to consider the match\n",
    "    max_gap_seconds=1, # gaps of prob < min_prob that could be inside segment\n",
    "    min_segment_seconds=3, # discard very short segments\n",
    "    smooth_fraction=0.02, # smoothing strength. Raw probabilities are smoothed to adapt to fluctuations between frames.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each video we get a list of segments:\n",
    "```\n",
    "[\n",
    "    ...\n",
    "    {\n",
    "        \"start_timestamp\": \"00:00:32.500\",\n",
    "        \"end_timestamp\": \"00:00:41.500\",\n",
    "        \"fps\": 29.97002997002997,\n",
    "        \"segment_info\": null, # not used with clip, but could be used with gpt4o\n",
    "        \"video_id\": \"KvRK5Owqzgw\"\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotaion step 1: extract information (clues) from transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-05 20:10:34.537595 EbOPpWi4L8s - started\n",
      "2024-08-05 20:10:34.538707 EbOPpWi4L8s part 0 - started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:30<01:31, 30.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-05 20:11:04.888972 EbOPpWi4L8s - done\n",
      "2024-08-05 20:11:04.889714 gcNh17Ckjgg - started\n",
      "2024-08-05 20:11:04.890555 gcNh17Ckjgg part 0 - started\n",
      "2024-08-05 20:11:42.035626 gcNh17Ckjgg part 1 - started\n",
      "2024-08-05 20:12:32.563929 gcNh17Ckjgg part 2 - started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [03:01<03:22, 101.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-05 20:13:35.748808 gcNh17Ckjgg - done\n",
      "2024-08-05 20:13:35.749525 bEv6CCg2BC8 - started\n",
      "2024-08-05 20:13:35.750599 bEv6CCg2BC8 part 0 - started\n",
      "2024-08-05 20:14:52.292449 bEv6CCg2BC8 part 1 - started\n",
      "2024-08-05 20:15:33.855359 bEv6CCg2BC8 part 2 - started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [05:37<02:06, 126.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-05 20:16:11.757339 bEv6CCg2BC8 - done\n",
      "2024-08-05 20:16:11.758068 xqvCmoLULNY - started\n",
      "2024-08-05 20:16:11.758798 xqvCmoLULNY part 0 - started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [05:54<00:00, 88.54s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-05 20:16:28.678385 xqvCmoLULNY - done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datagen import generate_clues\n",
    "\n",
    "human_prompt = \"\"\"\n",
    "The provided video is a tutorial about how to perform squats. \n",
    "\n",
    "I need to understand HOW THE PERSON SHOWN IN EACH SEGMENT PERFORMS SQUATS IN THIS SEGMENT.\n",
    "What is done correctly.\n",
    "What mistakes they make. Why these mistakes happen.\n",
    "How these mistakes could be improved.\n",
    "\n",
    "It is very improtant that the information that you provide would describe how the person shown in the segment is doing squats, and not some generic advice that is unrelated to the visual information.\n",
    "\"\"\"\n",
    "\n",
    "clues = generate_clues(\n",
    "    # video_ids=['byxWus7BwfQ'],\n",
    "    config=config,\n",
    "    human_prompt=human_prompt,\n",
    "    segments_per_call=5, # the output might be quite long, so need to limit number of segments per gpt call to respect max output legnth\n",
    "    raise_on_error=True, # interrupt when encountering an error. Useful for debugging.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotaion step 2: extract information from transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-05 20:19:08.236552 EbOPpWi4L8s - started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:06<00:18,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-05 20:19:14.411218 EbOPpWi4L8s - done\n",
      "2024-08-05 20:19:14.411934 gcNh17Ckjgg - started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:48<00:55, 27.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-05 20:19:57.170396 gcNh17Ckjgg - done\n",
      "2024-08-05 20:19:57.171090 bEv6CCg2BC8 - started\n",
      "2024-08-05 20:19:58.866695 5 validation errors for VideoAnnotation\n",
      "segments -> 0 -> segment_annotation\n",
      "  field required (type=value_error.missing)\n",
      "segments -> 1 -> segment_annotation\n",
      "  field required (type=value_error.missing)\n",
      "segments -> 2 -> segment_annotation\n",
      "  field required (type=value_error.missing)\n",
      "segments -> 3 -> segment_annotation\n",
      "  field required (type=value_error.missing)\n",
      "segments -> 4 -> segment_annotation\n",
      "  field required (type=value_error.missing)\n",
      "2024-08-05 20:19:58.866845 Error while generating annotations for bEv6CCg2BC8 part 0, skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [01:03<00:21, 21.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-05 20:20:11.281963 bEv6CCg2BC8 - done\n",
      "2024-08-05 20:20:11.282664 xqvCmoLULNY - started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:06<00:00, 16.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-05 20:20:14.596331 xqvCmoLULNY - done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datagen import generate_annotations\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "# This information that will be extracted for each segment from the transcript and data from the previous step.\n",
    "# This is the most important part for the annotation, and getting good results requires a lot of experimenting.\n",
    "\n",
    "\n",
    "human_prompt = '''\n",
    "You are given a JSON object that contains clues about segments of a video with timecodes.\n",
    "!!!! For each segment provided in a JSON object you need to answer on the following questions:\n",
    "1. Given the data found in the JSON object, what is a probability that this part contains a footage of a person doing squats? [the answer could be only \"high\", \"medium\", \"low\", or null (if impossible to infer from the provided data)]\n",
    "2. Given the data found in the JSON object and even if the answer on the previous question is \"low\", does this person do squats right, wrong, or mixed? [the answer could be only \"right\", \"wrong\", \"mixed\", or null (if impossible to infer from the provided data)]\n",
    "3. Given the data found in the JSON object, what exactly does thing person do right and/or wrong regarding their squats technique? [the answer should be clear and focused on body parts]\n",
    "4. If the answer on the previous question contains description of wrong technique, explain how to fix these mistakes using your \"own knowledge\" like you are a sports coach.\n",
    "'''\n",
    "\n",
    "class SegmentFeedback(BaseModel):\n",
    "    '''\n",
    "—> GOOD EXAMPLES:\n",
    "    \"wrong\":\"Knees caving in: This can stress the knees and reduce effectiveness\"\n",
    "    \"correction\":\"Focus on keeping knees aligned with your toes.\"\n",
    "    \"wrong\":\"Rounding the back: This increases the risk of back injuries\"\n",
    "    \"correction\":\"Keep your chest up and maintain a neutral spine throughout the movement.\"\n",
    "    \"wrong\":\"Heels are lifting off the ground: this shifts the weight forward, reducing stability\"\n",
    "    \"correction\":\" Keep your weight on your heels and press through them as you rise.\"\n",
    "    \"right\":\"Chest and shoulders: The chest is up, and the shoulders are back, maintaining an upright torso.\"\n",
    "    \"correction\":null\n",
    "—> BAD EXAMPLES:\n",
    "    \"wrong\":\"knees\"\n",
    "    \"correction\":\"fix knees\"\n",
    "    \"wrong\":\"back looks funny\"\n",
    "    \"correction\":\"make back better\"\n",
    "    \"wrong\":\"feet are doing something\"\n",
    "    \"correction\":\"feet should be different\"\n",
    "    \"right\":\"arms\"\n",
    "    \"correction\":\"arms are fine i think\"\n",
    "—> BAD EXAMPLES END HERE\n",
    "    '''\n",
    "    right: Optional[str] = Field(description='what was right in the performance')\n",
    "    wrong: Optional[str] = Field(description='what was wrong in the performance')\n",
    "    correction: Optional[str] = Field(description='how and in what ways it the performance could be improved')\n",
    "\n",
    "# The segment timestamps are taken from the provided information.\n",
    "class SegmentAnnotation(BaseModel):\n",
    "    squats_probability: Optional[str] = Field(description='how high is the probability that the person is doing squats in the segment: low, medium, high, unknown(null)')\n",
    "    squats_technique_correctness: Optional[str] = Field(description='correctness of the squat technique.')\n",
    "    squats_feedback: Optional[SegmentFeedback] = Field(description='what was right and wrong in the squat perfomance in the segment. When the technique is incorrect, provide instructions how to correct them.')\n",
    "\n",
    "# we will only take the segments where the \"doing_squats\" field is positive.\n",
    "annotations = generate_annotations(\n",
    "    human_prompt=human_prompt,\n",
    "    config=config,\n",
    "    segments_per_call=5,\n",
    "    annotation_schema=SegmentAnnotation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 31775.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total segments: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'end_timestamp': '00:00:13.250',\n",
       " 'segment_annotation': {'squats_feedback': {'correction': None,\n",
       "   'right': 'Use of chair for support: Good method for beginners to learn proper form without risk. Clear instructions on basic form steps: Ensures proper understanding of essential squat mechanics.',\n",
       "   'wrong': None},\n",
       "  'squats_probability': 'high',\n",
       "  'squats_technique_correctness': 'correct'},\n",
       " 'start_timestamp': '00:00:04.250',\n",
       " 'video_id': 'EbOPpWi4L8s',\n",
       " 'id': 'EbOPpWi4L8s_0',\n",
       " 'video_path': 'EbOPpWi4L8s_0.mp4'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datagen import aggregate_annotations\n",
    "\n",
    "def filter_annotations(ann):\n",
    "    if ann['squats_probability'] in [None, 'low', 'None', 'null']:\n",
    "        # if we're not able to infer probability or prob is low, we don't need it\n",
    "        return False\n",
    "    if ann['squats_technique_correctness'] in [None, 'null', 'None']:\n",
    "        # if we couldnt establish correctness at all, the feedback is probably useless\n",
    "        return False\n",
    "    if ann['squats_technique_correctness'] in ['mixed']:\n",
    "        # discard empty segment if correctness isn't clear since there isn't any information to use for training\n",
    "        if ann['squats_feedback'] is None:\n",
    "            return False\n",
    "        if set(ann['squats_feedback'].values()) == set([None]):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "annotations = aggregate_annotations(config, filter_func=filter_annotations, annotation_file='annotations.json')\n",
    "print('Total segments:', len(annotations))\n",
    "annotations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The last step is to cut video clips for annotated segments from original videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:27<00:00,  2.78s/it]\n"
     ]
    }
   ],
   "source": [
    "from datagen import cut_videos\n",
    "cut_videos(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as a result we generated:\n",
    "- `<data_dir>/clips/` with video clips that you can use for training\n",
    "- `<data_dir>/annotations.json` with list of items with fields:\n",
    "    - video_id: 11-char youtube video id (youtube.com/watch?v=<id>)\n",
    "    - start_timestamp/end_timestamp of the clip relative to the youtube video it's taken from\n",
    "    - video_path of the clip relative to `<data_dir>/clips/`\n",
    "    - segment_annotation that you can use for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
