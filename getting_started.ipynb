{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Data Generation SDK\n",
    "\n",
    "We are going to generate a dataset of squat videos with instructions how to perform them, so that we can train an AI pesonal trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from datagen import DatagenConfig\n",
    "# this config handles all the bookeeping so you need to pass it everywhere.\n",
    "config = DatagenConfig.from_yaml('./config.yaml')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a list of search queries to search for videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how to do squats properly',\n",
       " 'common mistakes when doing squats',\n",
       " 'correct form for squats',\n",
       " 'squat form mistakes',\n",
       " 'how to avoid squat mistakes']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datagen import get_queries\n",
    "queries = get_queries(\n",
    "    config=config,\n",
    "    prompt='I want to find videos where people show how to do squats and demonstrate popular mistakes.',\n",
    "    num_queries=5\n",
    ")\n",
    "queries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download video information for each query.\n",
    "\n",
    "We'll get 2 videos for each query.<br>\n",
    "One video might be found with multiple queries, so we might get less than `n_queries*videos_per_query` videos.<br>\n",
    "If you want to get all youtube videos for a query, don't pass `videos_per_query` parameter.\n",
    "\n",
    "You can limit the search to only videos licensed with Creative Commons (as indicated by youtube).<br>\n",
    "As this search isn't directly implemented in searching libraries yet, we search for all videos and filter for license afterwards.<br>\n",
    "Unfortunately, this way you will likely get very few results, so use with caution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:04<00:00,  1.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['bENvTGKtYAA',\n",
       " 'IB_icWRzi4E',\n",
       " 'gcNh17Ckjgg',\n",
       " 'W73Mc0Gil9A',\n",
       " 'bEv6CCg2BC8',\n",
       " 'byxWus7BwfQ',\n",
       " 'my0tLDaWyDU']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datagen import get_video_ids\n",
    "ids = get_video_ids(queries, config=config, videos_per_query=2, only_creative_commons=False)\n",
    "ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download videos and autogenerated subtitles\n",
    "\n",
    "You can change sub languages, formats etc with `yt_dlp_opts` dictionary (refer to https://github.com/yt-dlp/yt-dlp).<br>\n",
    "The SDK is expecting `.mp4` video files (for now), so don't change that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=bENvTGKtYAA\n",
      "[youtube] bENvTGKtYAA: Downloading webpage\n",
      "[youtube] bENvTGKtYAA: Downloading ios player API JSON\n",
      "[youtube] bENvTGKtYAA: Downloading tv player API JSON\n",
      "[youtube] bENvTGKtYAA: Downloading player d2e656ee\n",
      "[youtube] bENvTGKtYAA: Downloading m3u8 information\n",
      "[info] bENvTGKtYAA: Downloading subtitles: en\n",
      "[info] bENvTGKtYAA: Downloading 1 format(s): 18\n",
      "[info] Writing video subtitles to: tmp/squats/videos/bENvTGKtYAA.en.vtt\n",
      "[download] Destination: tmp/squats/videos/bENvTGKtYAA.en.vtt\n",
      "[download] 100% of   99.18KiB in 00:00:00 at 1.41MiB/s\n",
      "[download] Destination: tmp/squats/videos/bENvTGKtYAA.mp4\n",
      "[download] 100% of   18.00MiB in 00:00:00 at 19.92MiB/s    \n",
      "[MoveFiles] Moving file \"tmp/squats/videos/bENvTGKtYAA.en.vtt\" to \"tmp/squats/subs/bENvTGKtYAA.en.vtt\"\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=IB_icWRzi4E\n",
      "[youtube] IB_icWRzi4E: Downloading webpage\n",
      "[youtube] IB_icWRzi4E: Downloading ios player API JSON\n",
      "[youtube] IB_icWRzi4E: Downloading tv player API JSON\n",
      "[youtube] IB_icWRzi4E: Downloading m3u8 information\n",
      "[info] IB_icWRzi4E: Downloading subtitles: en\n",
      "[info] IB_icWRzi4E: Downloading 1 format(s): 18\n",
      "[info] Writing video subtitles to: tmp/squats/videos/IB_icWRzi4E.en.vtt\n",
      "[download] Destination: tmp/squats/videos/IB_icWRzi4E.en.vtt\n",
      "[download] 100% of   24.24KiB in 00:00:00 at 477.11KiB/s\n",
      "[download] Destination: tmp/squats/videos/IB_icWRzi4E.mp4\n",
      "[download] 100% of    4.95MiB in 00:00:00 at 8.85MiB/s     \n",
      "[MoveFiles] Moving file \"tmp/squats/videos/IB_icWRzi4E.en.vtt\" to \"tmp/squats/subs/IB_icWRzi4E.en.vtt\"\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=W73Mc0Gil9A\n",
      "[youtube] W73Mc0Gil9A: Downloading webpage\n",
      "[youtube] W73Mc0Gil9A: Downloading ios player API JSON\n",
      "[youtube] W73Mc0Gil9A: Downloading tv player API JSON\n",
      "[youtube] W73Mc0Gil9A: Downloading m3u8 information\n",
      "[info] W73Mc0Gil9A: Downloading subtitles: en\n",
      "[info] W73Mc0Gil9A: Downloading 1 format(s): 18\n",
      "[info] Writing video subtitles to: tmp/squats/videos/W73Mc0Gil9A.en.vtt\n",
      "[download] Destination: tmp/squats/videos/W73Mc0Gil9A.en.vtt\n",
      "[download] 100% of   78.14KiB in 00:00:00 at 892.10KiB/s\n",
      "[download] Destination: tmp/squats/videos/W73Mc0Gil9A.mp4\n",
      "[download] 100% of   19.39MiB in 00:00:01 at 18.77MiB/s    \n",
      "[MoveFiles] Moving file \"tmp/squats/videos/W73Mc0Gil9A.en.vtt\" to \"tmp/squats/subs/W73Mc0Gil9A.en.vtt\"\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=bEv6CCg2BC8\n",
      "[youtube] bEv6CCg2BC8: Downloading webpage\n",
      "[youtube] bEv6CCg2BC8: Downloading ios player API JSON\n",
      "[youtube] bEv6CCg2BC8: Downloading tv player API JSON\n",
      "[youtube] bEv6CCg2BC8: Downloading m3u8 information\n",
      "[info] bEv6CCg2BC8: Downloading subtitles: en\n",
      "[info] bEv6CCg2BC8: Downloading 1 format(s): 18\n",
      "[info] Writing video subtitles to: tmp/squats/videos/bEv6CCg2BC8.en.vtt\n",
      "[download] Destination: tmp/squats/videos/bEv6CCg2BC8.en.vtt\n",
      "[download] 100% of  111.79KiB in 00:00:00 at 1.91MiB/s\n",
      "[download] Destination: tmp/squats/videos/bEv6CCg2BC8.mp4\n",
      "[download] 100% of   21.82MiB in 00:00:01 at 18.78MiB/s    \n",
      "[MoveFiles] Moving file \"tmp/squats/videos/bEv6CCg2BC8.en.vtt\" to \"tmp/squats/subs/bEv6CCg2BC8.en.vtt\"\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=byxWus7BwfQ\n",
      "[youtube] byxWus7BwfQ: Downloading webpage\n",
      "[youtube] byxWus7BwfQ: Downloading ios player API JSON\n",
      "[youtube] byxWus7BwfQ: Downloading tv player API JSON\n",
      "[youtube] byxWus7BwfQ: Downloading m3u8 information\n",
      "[info] byxWus7BwfQ: Downloading subtitles: en\n",
      "[info] byxWus7BwfQ: Downloading 1 format(s): 18\n",
      "[info] Writing video subtitles to: tmp/squats/videos/byxWus7BwfQ.en.vtt\n",
      "[download] Destination: tmp/squats/videos/byxWus7BwfQ.en.vtt\n",
      "[download] 100% of   33.97KiB in 00:00:00 at 698.62KiB/s\n",
      "[download] Destination: tmp/squats/videos/byxWus7BwfQ.mp4\n",
      "[download] 100% of   11.12MiB in 00:00:00 at 11.36MiB/s    \n",
      "[MoveFiles] Moving file \"tmp/squats/videos/byxWus7BwfQ.en.vtt\" to \"tmp/squats/subs/byxWus7BwfQ.en.vtt\"\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=my0tLDaWyDU\n",
      "[youtube] my0tLDaWyDU: Downloading webpage\n",
      "[youtube] my0tLDaWyDU: Downloading ios player API JSON\n",
      "[youtube] my0tLDaWyDU: Downloading tv player API JSON\n",
      "[youtube] my0tLDaWyDU: Downloading m3u8 information\n",
      "[info] my0tLDaWyDU: Downloading subtitles: en\n",
      "[info] my0tLDaWyDU: Downloading 1 format(s): 18\n",
      "[info] Writing video subtitles to: tmp/squats/videos/my0tLDaWyDU.en.vtt\n",
      "[download] Destination: tmp/squats/videos/my0tLDaWyDU.en.vtt\n",
      "[download] 100% of   70.79KiB in 00:00:00 at 1.15MiB/s\n",
      "[download] Destination: tmp/squats/videos/my0tLDaWyDU.mp4\n",
      "[download] 100% of   16.91MiB in 00:00:01 at 16.22MiB/s    \n",
      "[MoveFiles] Moving file \"tmp/squats/videos/my0tLDaWyDU.en.vtt\" to \"tmp/squats/subs/my0tLDaWyDU.en.vtt\"\n"
     ]
    }
   ],
   "source": [
    "from datagen import download_videos\n",
    "download_videos(ids, config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect segments from video and analyze them with gpt4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "device = torch.device(\"mps\")\n",
    "#model = AutoModel.from_pretrained(\"google/siglip-so400m-patch14-384\").cuda()\n",
    "#processor = AutoProcessor.from_pretrained(\"google/siglip-so400m-patch14-384\")\n",
    "    \n",
    "model = AutoModel.from_pretrained(\"google/siglip-so400m-patch14-384\").to(device)\n",
    "processor = AutoProcessor.from_pretrained(\"google/siglip-so400m-patch14-384\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W73Mc0Gil9A - starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:12<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 5.84 GB, other allocations: 384.00 KB, max allowed: 9.07 GB). Tried to allocate 3.29 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     doing_squats: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m Field(description\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mWhether the person is doing squats. Only consider video of people, not renders or cartoons. If a person looks like they are preparing to do squats or standing between reps, consider them also doing squats if they are in a gym setting, wearing sportswear etc.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m     \u001b[39m# overlay_text: str = Field(description='Overlay text that is superimprosed over the image, if present.')\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m detect_segments_clip(\n\u001b[1;32m     15\u001b[0m     \u001b[39m# segment_info_schema=SegmentInfo,\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m     \u001b[39m# video_ids=['KvRK5Owqzgw'],\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m     text_prompts\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39ma person doing squats\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     18\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     19\u001b[0m     processor\u001b[39m=\u001b[39;49mprocessor,\n\u001b[1;32m     20\u001b[0m     fps_sampling\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m     21\u001b[0m     device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmps\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     22\u001b[0m     config\u001b[39m=\u001b[39;49mconfig\n\u001b[1;32m     23\u001b[0m )\n",
      "File \u001b[0;32m~/datagen_sdk/datagen/detect_segments.py:122\u001b[0m, in \u001b[0;36mdetect_segments_clip\u001b[0;34m(config, model, processor, device, video_ids, fps_sampling, text_prompts, min_duration, max_duration)\u001b[0m\n\u001b[1;32m    119\u001b[0m inputs \u001b[39m=\u001b[39m processor(text\u001b[39m=\u001b[39mtext_prompts, images\u001b[39m=\u001b[39mframes, padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    121\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 122\u001b[0m     outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m    124\u001b[0m logits_per_image \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits_per_image\n\u001b[1;32m    125\u001b[0m probs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msigmoid(logits_per_image)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/datagen_sdk/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/datagen_sdk/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/datagen_sdk/.conda/lib/python3.10/site-packages/transformers/models/siglip/modeling_siglip.py:1384\u001b[0m, in \u001b[0;36mSiglipModel.forward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, return_loss, output_attentions, output_hidden_states, return_dict, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m   1379\u001b[0m output_hidden_states \u001b[39m=\u001b[39m (\n\u001b[1;32m   1380\u001b[0m     output_hidden_states \u001b[39mif\u001b[39;00m output_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39moutput_hidden_states\n\u001b[1;32m   1381\u001b[0m )\n\u001b[1;32m   1382\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1384\u001b[0m vision_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvision_model(\n\u001b[1;32m   1385\u001b[0m     pixel_values\u001b[39m=\u001b[39;49mpixel_values,\n\u001b[1;32m   1386\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1387\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1388\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1389\u001b[0m     interpolate_pos_encoding\u001b[39m=\u001b[39;49minterpolate_pos_encoding,\n\u001b[1;32m   1390\u001b[0m )\n\u001b[1;32m   1392\u001b[0m text_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_model(\n\u001b[1;32m   1393\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1394\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1398\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1399\u001b[0m )\n\u001b[1;32m   1401\u001b[0m image_embeds \u001b[39m=\u001b[39m vision_outputs[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/datagen_sdk/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/datagen_sdk/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/datagen_sdk/.conda/lib/python3.10/site-packages/transformers/models/siglip/modeling_siglip.py:1087\u001b[0m, in \u001b[0;36mSiglipVisionTransformer.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m   1082\u001b[0m output_hidden_states \u001b[39m=\u001b[39m (\n\u001b[1;32m   1083\u001b[0m     output_hidden_states \u001b[39mif\u001b[39;00m output_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39moutput_hidden_states\n\u001b[1;32m   1084\u001b[0m )\n\u001b[1;32m   1085\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1087\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(pixel_values, interpolate_pos_encoding\u001b[39m=\u001b[39;49minterpolate_pos_encoding)\n\u001b[1;32m   1089\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m   1090\u001b[0m     inputs_embeds\u001b[39m=\u001b[39mhidden_states,\n\u001b[1;32m   1091\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   1092\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1093\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1094\u001b[0m )\n\u001b[1;32m   1096\u001b[0m last_hidden_state \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/datagen_sdk/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/datagen_sdk/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/datagen_sdk/.conda/lib/python3.10/site-packages/transformers/models/siglip/modeling_siglip.py:310\u001b[0m, in \u001b[0;36mSiglipVisionEmbeddings.forward\u001b[0;34m(self, pixel_values, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, pixel_values: torch\u001b[39m.\u001b[39mFloatTensor, interpolate_pos_encoding\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    309\u001b[0m     _, _, height, width \u001b[39m=\u001b[39m pixel_values\u001b[39m.\u001b[39mshape\n\u001b[0;32m--> 310\u001b[0m     patch_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpatch_embedding(pixel_values)  \u001b[39m# shape = [*, width, grid, grid]\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     embeddings \u001b[39m=\u001b[39m patch_embeds\u001b[39m.\u001b[39mflatten(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    313\u001b[0m     \u001b[39mif\u001b[39;00m interpolate_pos_encoding:\n",
      "File \u001b[0;32m~/datagen_sdk/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/datagen_sdk/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/datagen_sdk/.conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/datagen_sdk/.conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    455\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 5.84 GB, other allocations: 384.00 KB, max allowed: 9.07 GB). Tried to allocate 3.29 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "from datagen import detect_segments_clip\n",
    "\n",
    "from typing import Optional\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# This is the schema that we will extract from each detected segment.\n",
    "# \"doing_squats\" will be used for filtering and \"overlay_text\" for annotation.\n",
    "\n",
    "class SegmentInfo(BaseModel):\n",
    "    '''Information about a segment'''\n",
    "    doing_squats: bool = Field(description='Whether the person is doing squats. Only consider video of people, not renders or cartoons. If a person looks like they are preparing to do squats or standing between reps, consider them also doing squats if they are in a gym setting, wearing sportswear etc.')\n",
    "    # overlay_text: str = Field(description='Overlay text that is superimprosed over the image, if present.')\n",
    "\n",
    "detect_segments_clip(\n",
    "    # segment_info_schema=SegmentInfo,\n",
    "    # video_ids=['KvRK5Owqzgw'],\n",
    "    text_prompts='a person doing squats',\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    fps_sampling=2,\n",
    "    device=\"mps\",\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each video we get a list of segments:\n",
    "```\n",
    "[\n",
    "    ...\n",
    "    {\n",
    "        \"start_timestamp\": \"00:00:31.198\",\n",
    "        \"end_timestamp\": \"00:00:36.003\",\n",
    "        \"fps\": 29.97002997002997,\n",
    "        \"segment_info\": {\n",
    "            \"doing_squats\": true,\n",
    "            \"overlay_text\": \"HIP-WIDTH APART\"\n",
    "        },\n",
    "        \"video_id\": \"gcNh17Ckjgg\"\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate the segments from trascript + additional info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datagen.annotate import generate_annotations, generate_clues\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "human_prompt = \"\"\"User's instructions:\n",
    "The initial video was a tutorial about how to perform squats. \n",
    "I need to restore what happened in specific *parts* of this video.\n",
    "\n",
    "You'll find timecodes for the *parts* I'm interested in below. \n",
    "\n",
    "All *PARTS* CONTAIN A PERSON DOING SQUATS.\n",
    "\n",
    "I NEED YOU TO DELIVER CLUES THAT WILL HELP ME RESTORE INFORMATION ABOUT HOW THIS PERSON PERFORMS SQUATS IN THIS SPECIFIC *PART*. \n",
    "\n",
    "!!!I need to restore data about HOW THIS PERSON PERFORMS SQUATS. \n",
    "What mistakes they make. What improvements they show. \n",
    "What they do correctly. What could be improved.!!!\n",
    "\n",
    "Please, help me find relevant clues to reconstruct this information for each provided *part*.\n",
    "\n",
    "Here is what I expect to have from you:\n",
    "1. *Local clues* that could help me guess how a person in a *part* of the initial video performs squats  \n",
    "2. *Global clues* that could help me guess how a person in a *part* of the initial video performs squats \n",
    "3. *Logical inferences* that could help me guess how a person in a *part* of the initial video performs squats \n",
    "\n",
    "!!!IT IS EXTREMELY IMPORTANT TO DELIVER ALL THREE THINGS!!!\n",
    "\n",
    "CLUES: A *clue*, in the context of reconstructing narratives from damaged data, \n",
    "is a fragment of information extracted from a corrupted or incomplete source that provides \n",
    "insight into the original content. These fragments serve as starting points for inference \n",
    "and deduction, allowing researchers to hypothesize about the fuller context or meaning of \n",
    "the degraded material. The process of identifying and interpreting clues involves both objective analysis of the \n",
    "available data and subjective extrapolation based on domain knowledge, contextual understanding, \n",
    "and logical reasoning.\n",
    "\n",
    "- LOCAL CLUES: THEY ARE LOCATED VERY CLOSE TO THE *PART* YOU ARE WORKING WITH REGARDING TIMESTAMPS\n",
    "- GLOBAL CLUES: THEY ARE SCATTERED ACROSS THE ENTIRE TRANSCRIPT\n",
    "\n",
    "LOGICAL INFERENCES: *Logical inference*, in the process of reconstructing narratives \n",
    "or information from damaged data, is the act of deriving plausible conclusions \n",
    "or filling in gaps based on available clues and contextual knowledge. This cognitive process \n",
    "involves applying deductive, inductive, or abductive reasoning to extrapolate beyond the explicit \n",
    "information provided by the damaged source. Logical inference relies on a combination of factual \n",
    "understanding, domain expertise, and analytical thinking to form connections between disparate \n",
    "pieces of information and generate coherent hypotheses about the missing or corrupted content. \n",
    "It often necessitates considering multiple possibilities, weighing probabilities, and making \n",
    "educated assumptions while maintaining awareness of potential biases or limitations in the \n",
    "reasoning process. The strength and validity of logical inferences can vary based on the quality  \n",
    "and quantity of available clues, the complexity of the subject matter, and the inferrer's expertise,\n",
    "making it both a powerful tool for information reconstruction and a process that requires careful \n",
    "scrutiny and validation.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class LocalClue(BaseModel):\n",
    "    '''\n",
    "        Good local clues examples: [\n",
    "      {\n",
    "        \"id\": \"LC1\",\n",
    "        \"timestamp\": \"00:00:19\",\n",
    "        \"quote\": \"exercises do them wrong and instead of\",\n",
    "        \"clue\": \"This phrase introduces the concept of incorrect exercise form, setting the stage for a demonstration of improper technique.\"\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"LC2\",\n",
    "        \"timestamp\": \"00:00:21\",\n",
    "        \"quote\": \"growing nice quads and glutes you'll\",\n",
    "        \"clue\": \"Mentions the expected benefits of proper squats (muscle growth), implying that these benefits won't be achieved with incorrect form.\"\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"LC3\",\n",
    "        \"timestamp\": \"00:00:22\",\n",
    "        \"quote\": \"feel aches and pains in your knees your\",\n",
    "        \"clue\": \"Directly states negative consequences of improper form, strongly suggesting that this segment demonstrates incorrect technique.\"\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"LC4\",\n",
    "        \"timestamp\": \"00:00:24\",\n",
    "        \"quote\": \"lower back and even your shoulders\",\n",
    "        \"clue\": \"Continuation of LC3, emphasizing multiple areas of potential pain from improper form.\"\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"LC5\",\n",
    "        \"timestamp\": \"00:00:26\",\n",
    "        \"quote\": \"let's see how to do it correctly\",\n",
    "        \"clue\": \"This phrase suggests a transition is about to occur. The incorrect form has been shown, and correct form will follow.\"\n",
    "      }\n",
    "    ]\n",
    "    '''\n",
    "    id: str = Field(description='LC1,LC2...')\n",
    "    timestamp: str = Field(description='the timestamp that is most probable for the clue')\n",
    "    quote: str = Field(description='the quote from the transcript that was used to create this clue')\n",
    "    clue: str = Field(description='the main clue data')\n",
    "    \n",
    "class GlobalClue(BaseModel):\n",
    "    '''\n",
    "    Good global clues examples: [\n",
    "      {\n",
    "        \"id\": \"GC1\",\n",
    "        \"timestamp\": \"00:01:15\",\n",
    "        \"quote\": \"Before we dive into specific techniques, let's talk about safety.\",\n",
    "        \"clue\": \"Introduces the theme of safety in squatting.\",\n",
    "        \"relevance_to_segment\": \"This earlier emphasis on safety provides context for why proper depth is important and why it's being addressed in our segment. It connects to the fear of knee pain mentioned in LC3.\"\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"GC2\",\n",
    "        \"timestamp\": \"00:02:30\",\n",
    "        \"quote\": \"Squatting is a fundamental movement pattern in everyday life.\",\n",
    "        \"clue\": \"Emphasizes the importance of squats beyond just exercise.\",\n",
    "        \"relevance_to_segment\": \"This broader context heightens the importance of learning proper squat depth as demonstrated in our segment. It suggests that the techniques shown have applications beyond just gym workouts.\"\n",
    "      },\n",
    "      {\n",
    "        \"clue_id\": \"GC3\",\n",
    "        \"timestamp\": \"00:05:20\",\n",
    "        \"quote\": \"If you have existing knee issues, consult a physician before attempting deep squats.\",\n",
    "        \"clue\": \"Provides a health disclaimer related to squat depth.\",\n",
    "        \"relevance_to_segment\": \"While this comes after our segment, it's relevant because it addresses the concern about knee pain mentioned in LC3. It suggests that the demonstration in our segment is generally safe but acknowledges individual variations.\"\n",
    "      },\n",
    "      {\n",
    "        \"clue_id\": \"GC4\",\n",
    "        \"timestamp\": \"00:06:45\",\n",
    "        \"quote\": \"Proper depth ensures full engagement of your quadriceps and glutes.\",\n",
    "        \"clue\": \"Explains the benefit of correct squat depth.\",\n",
    "        \"relevance_to_segment\": \"This later explanation provides justification for the depth guideline given in LC4. It helps viewers understand why the demonstrated technique is important.\"\n",
    "      },\n",
    "      {\n",
    "        \"clue_id\": \"GC5\",\n",
    "        \"timestamp\": \"00:00:30\",\n",
    "        \"quote\": \"Today, we'll cover squat variations for beginners to advanced lifters.\",\n",
    "        \"clue\": \"Outlines the scope of the entire video.\",\n",
    "        \"relevance_to_segment\": \"This early statement suggests that our segment, focusing on proper depth, is part of a comprehensive guide. It implies that the demonstration might be adaptable for different skill levels.\"\n",
    "      }\n",
    "    ]\n",
    "    '''\n",
    "    id: str = Field(description='GC1,GC2...')\n",
    "    timestamp: str = Field(description='the timestamp that is most probable for the clue')\n",
    "    quote: str = Field(description='the quote from the transcript that was used to create this clue')\n",
    "    clue: str = Field(description='the main clue data')\n",
    "    relevance_to_segment: str = Field(description='why do you think this global clue is relevant to the *part* you are working with right now')\n",
    "\n",
    "class AdditionalInformation(BaseModel):\n",
    "    '''\n",
    "    Good logical inference examples:\n",
    "    [\n",
    "      {\n",
    "        \"id\": \"LI1\",\n",
    "        \"description\": \"Primary Demonstration of Heel Lift\",\n",
    "        \"details\": \"Given that GC1-GC3 describe the 'most common mistake' as heels lifting off the ground, and this description immediately precedes our segment, it's highly probable that this is the primary error being demonstrated. This is further supported by the segment's focus on incorrect form (LC1-LC4).\"\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"LI2\",\n",
    "        \"description\": \"Multiple Error Demonstration\",\n",
    "        \"details\": \"While heel lift is likely the primary focus, the mention of multiple pain points (knees, lower back, shoulders in LC3-LC4) suggests that the demonstrator may be exhibiting several forms of incorrect technique simultaneously. This comprehensive 'what not to do' approach would be pedagogically effective.\"\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"LI3\",\n",
    "        \"description\": \"Possible Inclusion of 'Butt Wink'\",\n",
    "        \"details\": \"Although 'butt wink' is mentioned after our segment (GC4-GC6), its connection to back pain (which is mentioned in LC4) raises the possibility that this error is also present in the demonstration. The instructor may be showing multiple errors early on, then breaking them down individually later.\"\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"LI4\",\n",
    "        \"description\": \"Segment Placement in Overall Video Structure\",\n",
    "        \"details\": \"The segment's position (starting at 00:00:19) and the phrase 'let's see how to do it correctly' (LC5) at the end suggest this is an early, foundational part of the video. It likely serves to grab attention by showing common mistakes before transitioning to proper form instruction.\"\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"LI5\",\n",
    "        \"description\": \"Intentional Exaggeration of Errors\",\n",
    "        \"details\": \"Given the educational nature of the video, it's plausible that the demonstrator is intentionally exaggerating the incorrect form. This would make the errors more obvious to viewers and enhance the contrast with correct form shown later.\"\n",
    "      }\n",
    "    ]\n",
    "    '''\n",
    "    id: str = Field(description='LI1,LI2,...')\n",
    "    description: str = Field(description='A concise form of the logical inference')\n",
    "    details: str = Field(description='A verbose explanation of what insight about what happens in this *part* should be made based on delivered clues')\n",
    "\n",
    "class SegmentAnnotation(BaseModel):\n",
    "    local_clues: list[LocalClue] = Field(description='''Local clues are positioned very close to the *part* of the video in \n",
    "                                              terms of timestamps.''')\n",
    "    global_clues: list[GlobalClue] = Field(description='''Global clues are scattered across the entire transcript. Be very carefull with\n",
    "                                              them as it's very easy to accidentally assume wrong global clue because of limited attention or IQ. \n",
    "                                              ''')\n",
    "    logical_inferences: list[AdditionalInformation] = Field(description='''What guess about how a person performs squats in this *part* can we make based on clues''')\n",
    "\n",
    "# we will only take the segments where the \"doing_squats\" field is positive.\n",
    "clues = generate_clues(\n",
    "    config=config,\n",
    "    annotation_schema=SegmentAnnotation,\n",
    "    human_prompt=human_prompt,\n",
    "    segments_per_call=5,\n",
    "    raise_on_error=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datagen.annotate import generate_annotations, generate_clues\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "# This information that will be extracted for each segment from the transcript and data from the previous step.\n",
    "# This is the most important part for the annotation, and getting good results requires a lot of experimenting.\n",
    "\n",
    "import inspect\n",
    "\n",
    "human_prompt = '''\n",
    "You are given a JSON object that contains clues about segments of a video with timecodes.\n",
    "!!!! For each segment provided in a JSON object you need to answer on the following questions:\n",
    "1. Given the data found in the JSON object, what is a probability that this part contains a footage of a person doing squats? [the answer could be only \"High\",\"Medium\" or \"Low\"]\n",
    "2. Given the data found in the JSON object and even if the answer on the previous question is \"Low\", does this person do squats right, wrong, or mixed? [the answer could be only \"Right\", \"Wrong\", and \"Mixed\"]\n",
    "3. Given the data found in the JSON object, what exactly does thing person do right and/or wrong regarding their squats technique? [the answer should be clear and focused on body parts]\n",
    "4. If the answer on the previous question contains description of wrong technique, explain how to fix these mistakes using your \"own knowledge\" like you are a sports coach.\n",
    "'''\n",
    "\n",
    "class SegmentFeedback(BaseModel):\n",
    "    '''\n",
    "—> GOOD EXAMPLES:\n",
    "    \"wrong\":\"Knees caving in: This can stress the knees and reduce effectiveness\"\n",
    "    \"correction\":\"Focus on keeping knees aligned with your toes.\"\n",
    "    \"wrong\":\"Rounding the back: This increases the risk of back injuries\"\n",
    "    \"correction\":\"Keep your chest up and maintain a neutral spine throughout the movement.\"\n",
    "    \"wrong\":\"Heels are lifting off the ground: this shifts the weight forward, reducing stability\"\n",
    "    \"correction\":\" Keep your weight on your heels and press through them as you rise.\"\n",
    "    \"right\":\"Chest and shoulders: The chest is up, and the shoulders are back, maintaining an upright torso.\"\n",
    "    \"correction\":null\n",
    "—> BAD EXAMPLES:\n",
    "    \"wrong\":\"knees\"\n",
    "    \"correction\":\"fix knees\"\n",
    "    \"wrong\":\"back looks funny\"\n",
    "    \"correction\":\"make back better\"\n",
    "    \"wrong\":\"feet are doing something\"\n",
    "    \"correction\":\"feet should be different\"\n",
    "    \"right\":\"arms\"\n",
    "    \"correction\":\"arms are fine i think\"\n",
    "—> BAD EXAMPLES END HERE\n",
    "    '''\n",
    "    right: Optional[str] = Field(description='what was right in the performance')\n",
    "    wrong: Optional[str] = Field(description='what was wrong in the performance')\n",
    "    correction: Optional[str] = Field(description='how and in what ways it the performance could be improved')\n",
    "\n",
    "# The segment timestamps are taken from the provided information.\n",
    "class SegmentAnnotation(BaseModel):\n",
    "    '''\n",
    "Here is a JSON object that contains data about parts with timecodes of a video file that's called \"How to do squats: rights and wrongs\".\n",
    "                !!!! Answer on the following questions:\n",
    "                1. Given the data found in the JSON object, what is a propability that this part contains a footage of a person doing squats? [the answer could be only \"High\",\"Medium\" or \"Low\"]\n",
    "                2. Given the data found in the JSON object and even if the answer on the previous question is \"Low\", does this person do squats right, wrong, or mixed? [the answer could be only \"Right\", \"Wrong\", and \"Mixed\"]\n",
    "                3. Given the data found in the JSON object, what exactly does thing person do right and/or wrong regarding their squats technique? [the answer should be clear and focused on body parts]\n",
    "                4. If the answer on the previous question contains description of wrong technique, explain how to fix these mistakes using your \"own knowledge\" like you are a sports coach.\n",
    "\n",
    "    '''\n",
    "    squats_probability: Optional[str] = Field(description='how high is the probability that the person is doing squats in the segment: low, medium, high, unknown(null)')\n",
    "    squats_technique_correctness: Optional[bool] = Field(description='bollean correctness of the squat technique.')\n",
    "    squats_feedback: SegmentFeedback = Field(description='what was right and wrong in the squat perfomance in the segment. When the technique is incorrect, provide instructions how to correct them.')\n",
    "\n",
    "# we will only take the segments where the \"doing_squats\" field is positive.\n",
    "annotations = generate_annotations(\n",
    "    # human_prompt=human_prompt,\n",
    "    config=config,\n",
    "    segments_per_call=5,\n",
    "    annotation_schema=SegmentAnnotation,\n",
    "    # filter_by='doing_squats'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get a list of annotations for each video:\n",
    "```\n",
    "[\n",
    "    {\n",
    "        \"start_timestamp\": \"00:00:51.760\",\n",
    "        \"end_timestamp\": \"00:01:01.520\",\n",
    "        \"segment_annotation\": {\n",
    "            \"correct\": null,\n",
    "            \"incorrect_reasons\": null,\n",
    "            \"qa\": [\n",
    "                {\n",
    "                    \"question\": \"Was there important advice about performing the exercise correctly?\",\n",
    "                    \"answer\": \"Yes, the advice was to make sure the knees do not go forward of the toes.\",\n",
    "                    \"quote\": \"making sure that your knees do not go forward of your toes\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datagen import aggregate_annotations\n",
    "\n",
    "# saved to annotations.json\n",
    "annotations = aggregate_annotations(config)\n",
    "print('Total segments:', len(annotations))\n",
    "annotations[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The last step is to cut video clips for annotated segments from original videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datagen import cut_videos\n",
    "cut_videos(config=config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as a result we generated:\n",
    "- `<data_dir>/clips/` with video clips that you can use for training\n",
    "- `<data_dir>/annotations.json` with list of items with fields:\n",
    "    - video_id: 11-char youtube video id (youtube.com/watch?v=<id>)\n",
    "    - start_timestamp/end_timestamp of the clip relative to the youtube video it's taken from\n",
    "    - video_path of the clip relative to `<data_dir>/clips/`\n",
    "    - segment_annotation that you can use for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
