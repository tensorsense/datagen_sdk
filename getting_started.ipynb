{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Data Generation SDK\n",
    "\n",
    "We are going to generate a dataset of squat videos with instructions how to perform them, so that we can train an AI pesonal trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from datagen import DatagenConfig\n",
    "# this config handles all the bookeeping so you need to pass it everywhere.\n",
    "config = DatagenConfig.from_yaml('./config.yaml')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a list of search queries to search for videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how to do squats instructions', 'squat exercise tutorial']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datagen import get_queries\n",
    "queries = get_queries(\n",
    "    config=config,\n",
    "    prompt='I want to find instructional videos about how to do squats.',\n",
    "    num_queries=2\n",
    ")\n",
    "queries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download video information for each query.\n",
    "\n",
    "We'll get 2 videos for each query.<br>\n",
    "One video might be found with multiple queries, so we might get less than `n_queries*videos_per_query` videos.<br>\n",
    "If you want to get all youtube videos for a query, don't pass `videos_per_query` parameter.\n",
    "\n",
    "You can limit the search to only videos licensed with Creative Commons (as indicated by youtube).<br>\n",
    "As this search isn't directly implemented in searching libraries yet, we search for all videos and filter for license afterwards.<br>\n",
    "Unfortunately, this way you will likely get very few results, so use with caution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['xqvCmoLULNY', 'gcNh17Ckjgg', '4KmY44Xsg2w']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datagen import get_video_ids\n",
    "ids = get_video_ids(queries, config=config, videos_per_query=2, only_creative_commons=False)\n",
    "ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download videos and autogenerated subtitles\n",
    "\n",
    "You can change sub languages, formats etc with `yt_dlp_opts` dictionary (refer to https://github.com/yt-dlp/yt-dlp).<br>\n",
    "The SDK is expecting `.mp4` video files (for now), so don't change that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=YaXPRqUwItQ\n",
      "[youtube] YaXPRqUwItQ: Downloading webpage\n",
      "[youtube] YaXPRqUwItQ: Downloading ios player API JSON\n",
      "[youtube] YaXPRqUwItQ: Downloading player d2e656ee\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] YaXPRqUwItQ: nsig extraction failed: Some formats may be missing\n",
      "         n = U5NBYLtIN_DLGvfh ; player = https://www.youtube.com/s/player/d2e656ee/player_ias.vflset/en_US/base.js\n",
      "WARNING: [youtube] YaXPRqUwItQ: nsig extraction failed: Some formats may be missing\n",
      "         n = r2iA-HEoE7SU89EC ; player = https://www.youtube.com/s/player/d2e656ee/player_ias.vflset/en_US/base.js\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] YaXPRqUwItQ: Downloading m3u8 information\n"
     ]
    }
   ],
   "source": [
    "from datagen import download_videos\n",
    "download_videos(['gcNh17Ckjgg', 'KvRK5Owqzgw', 'xqvCmoLULNY', 'YaXPRqUwItQ'], config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect segments from video and analyze them with gpt4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb98ccf71e00478b821c0d2d44587564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/576 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217d42aded42447b8ea8cee473f18436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.51G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoProcessor, AutoModel\n\u001b[0;32m----> 3\u001b[0m model \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mgoogle/siglip-so400m-patch14-384\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m      4\u001b[0m processor \u001b[39m=\u001b[39m AutoProcessor\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mgoogle/siglip-so400m-patch14-384\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:2766\u001b[0m, in \u001b[0;36mPreTrainedModel.cuda\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2761\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2762\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCalling `cuda()` is not supported for `4-bit` or `8-bit` quantized models. Please use the model as it is, since the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2763\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2764\u001b[0m     )\n\u001b[1;32m   2765\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2766\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcuda(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    899\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \n\u001b[1;32m    901\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    914\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 915\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(\u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39mcuda(device))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         module\u001b[39m.\u001b[39m_apply(fn)\n\u001b[1;32m    781\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         module\u001b[39m.\u001b[39m_apply(fn)\n\u001b[1;32m    781\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         module\u001b[39m.\u001b[39m_apply(fn)\n\u001b[1;32m    781\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 804\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    805\u001b[0m p_should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    807\u001b[0m \u001b[39m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    899\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \n\u001b[1;32m    901\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    914\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 915\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(\u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39mcuda(device))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:284\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    283\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m\"\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 284\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    285\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    287\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"google/siglip-so400m-patch14-384\").cuda()\n",
    "processor = AutoProcessor.from_pretrained(\"google/siglip-so400m-patch14-384\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datagen import detect_segments_clip\n",
    "\n",
    "from typing import Optional\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# This is the schema that we will extract from each detected segment.\n",
    "# \"doing_squats\" will be used for filtering and \"overlay_text\" for annotation.\n",
    "\n",
    "class SegmentInfo(BaseModel):\n",
    "    '''Information about a segment'''\n",
    "    doing_squats: bool = Field(description='Whether the person is doing squats. Only consider video of people, not renders or cartoons. If a person looks like they are preparing to do squats or standing between reps, consider them also doing squats if they are in a gym setting, wearing sportswear etc.')\n",
    "    # overlay_text: str = Field(description='Overlay text that is superimprosed over the image, if present.')\n",
    "\n",
    "detect_segments_clip(\n",
    "    # segment_info_schema=SegmentInfo,\n",
    "    # video_ids=['KvRK5Owqzgw'],\n",
    "    text_prompts='a person doing squats',\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    fps_sampling=2,\n",
    "    device='cuda',\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each video we get a list of segments:\n",
    "```\n",
    "[\n",
    "    ...\n",
    "    {\n",
    "        \"start_timestamp\": \"00:00:31.198\",\n",
    "        \"end_timestamp\": \"00:00:36.003\",\n",
    "        \"fps\": 29.97002997002997,\n",
    "        \"segment_info\": {\n",
    "            \"doing_squats\": true,\n",
    "            \"overlay_text\": \"HIP-WIDTH APART\"\n",
    "        },\n",
    "        \"video_id\": \"gcNh17Ckjgg\"\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate the segments from trascript + additional info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xqvCmoLULNY - started\n",
      "xqvCmoLULNY part 0 - started\n",
      "xqvCmoLULNY part 1 - started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:20<01:02, 20.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xqvCmoLULNY - done\n",
      "gcNh17Ckjgg - started\n",
      "gcNh17Ckjgg part 0 - started\n",
      "gcNh17Ckjgg part 1 - started\n",
      "gcNh17Ckjgg part 2 - started\n",
      "gcNh17Ckjgg part 3 - started\n",
      "gcNh17Ckjgg part 4 - started\n",
      "gcNh17Ckjgg part 5 - started\n",
      "gcNh17Ckjgg part 6 - started\n",
      "gcNh17Ckjgg part 7 - started\n",
      "gcNh17Ckjgg part 8 - started\n",
      "gcNh17Ckjgg part 9 - started\n",
      "gcNh17Ckjgg part 10 - started\n",
      "gcNh17Ckjgg part 11 - started\n",
      "gcNh17Ckjgg part 12 - started\n",
      "gcNh17Ckjgg part 13 - started\n",
      "gcNh17Ckjgg part 14 - started\n",
      "gcNh17Ckjgg part 15 - started\n",
      "gcNh17Ckjgg part 16 - started\n",
      "gcNh17Ckjgg part 17 - started\n",
      "gcNh17Ckjgg part 18 - started\n",
      "gcNh17Ckjgg part 19 - started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [08:50<10:16, 308.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcNh17Ckjgg - done\n",
      "EbOPpWi4L8s - started\n",
      "EbOPpWi4L8s part 0 - started\n",
      "EbOPpWi4L8s part 1 - started\n",
      "EbOPpWi4L8s part 2 - started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [09:41<03:10, 190.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EbOPpWi4L8s - done\n",
      "HFnSsLIB7a4 - started\n",
      "HFnSsLIB7a4 part 0 - started\n",
      "HFnSsLIB7a4 part 1 - started\n",
      "HFnSsLIB7a4 part 2 - started\n",
      "HFnSsLIB7a4 part 3 - started\n",
      "HFnSsLIB7a4 part 4 - started\n",
      "HFnSsLIB7a4 part 5 - started\n",
      "HFnSsLIB7a4 part 6 - started\n",
      "HFnSsLIB7a4 part 7 - started\n",
      "HFnSsLIB7a4 part 8 - started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [11:30<00:00, 172.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HFnSsLIB7a4 - done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datagen.annotate import generate_annotations, generate_clues\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "human_prompt = \"\"\"User's instructions:\n",
    "The initial video was a tutorial about how to perform squats. \n",
    "I need to restore what happened in specific *parts* of this video.\n",
    "\n",
    "You'll find timecodes for the *parts* I'm interested in below. \n",
    "\n",
    "All *PARTS* CONTAIN A PERSON DOING SQUATS.\n",
    "\n",
    "I NEED YOU TO DELIVER CLUES THAT WILL HELP ME RESTORE INFORMATION ABOUT HOW THIS PERSON PERFORMS SQUATS IN THIS SPECIFIC *PART*. \n",
    "\n",
    "!!!I need to restore data about HOW THIS PERSON PERFORMS SQUATS. \n",
    "What mistakes they make. What improvements they show. \n",
    "What they do correctly. What could be improved.!!!\n",
    "\n",
    "Please, help me find relevant clues to reconstruct this information for each provided *part*.\n",
    "\n",
    "Here is what I expect to have from you:\n",
    "1. *Local clues* that could help me guess how a person in a *part* of the initial video performs squats  \n",
    "2. *Global clues* that could help me guess how a person in a *part* of the initial video performs squats \n",
    "3. *Logical inferences* that could help me guess how a person in a *part* of the initial video performs squats \n",
    "\n",
    "!!!IT IS EXTREMELY IMPORTANT TO DELIVER ALL THREE THINGS!!!\n",
    "\n",
    "CLUES: A *clue*, in the context of reconstructing narratives from damaged data, \n",
    "is a fragment of information extracted from a corrupted or incomplete source that provides \n",
    "insight into the original content. These fragments serve as starting points for inference \n",
    "and deduction, allowing researchers to hypothesize about the fuller context or meaning of \n",
    "the degraded material. The process of identifying and interpreting clues involves both objective analysis of the \n",
    "available data and subjective extrapolation based on domain knowledge, contextual understanding, \n",
    "and logical reasoning.\n",
    "\n",
    "- LOCAL CLUES: THEY ARE LOCATED VERY CLOSE TO THE *PART* YOU ARE WORKING WITH REGARDING TIMESTAMPS\n",
    "- GLOBAL CLUES: THEY ARE SCATTERED ACROSS THE ENTIRE TRANSCRIPT\n",
    "\n",
    "LOGICAL INFERENCES: *Logical inference*, in the process of reconstructing narratives \n",
    "or information from damaged data, is the act of deriving plausible conclusions \n",
    "or filling in gaps based on available clues and contextual knowledge. This cognitive process \n",
    "involves applying deductive, inductive, or abductive reasoning to extrapolate beyond the explicit \n",
    "information provided by the damaged source. Logical inference relies on a combination of factual \n",
    "understanding, domain expertise, and analytical thinking to form connections between disparate \n",
    "pieces of information and generate coherent hypotheses about the missing or corrupted content. \n",
    "It often necessitates considering multiple possibilities, weighing probabilities, and making \n",
    "educated assumptions while maintaining awareness of potential biases or limitations in the \n",
    "reasoning process. The strength and validity of logical inferences can vary based on the quality  \n",
    "and quantity of available clues, the complexity of the subject matter, and the inferrer's expertise,\n",
    "making it both a powerful tool for information reconstruction and a process that requires careful \n",
    "scrutiny and validation.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class LocalClue(BaseModel):\n",
    "    '''\n",
    "        Good local clues examples: [\n",
    "      {\n",
    "        \"id\": \"LC1\",\n",
    "        \"timestamp\": \"00:00:19\",\n",
    "        \"quote\": \"exercises do them wrong and instead of\",\n",
    "        \"clue\": \"This phrase introduces the concept of incorrect exercise form, setting the stage for a demonstration of improper technique.\"\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"LC2\",\n",
    "        \"timestamp\": \"00:00:21\",\n",
    "        \"quote\": \"growing nice quads and glutes you'll\",\n",
    "        \"clue\": \"Mentions the expected benefits of proper squats (muscle growth), implying that these benefits won't be achieved with incorrect form.\"\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"LC3\",\n",
    "        \"timestamp\": \"00:00:22\",\n",
    "        \"quote\": \"feel aches and pains in your knees your\",\n",
    "        \"clue\": \"Directly states negative consequences of improper form, strongly suggesting that this segment demonstrates incorrect technique.\"\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"LC4\",\n",
    "        \"timestamp\": \"00:00:24\",\n",
    "        \"quote\": \"lower back and even your shoulders\",\n",
    "        \"clue\": \"Continuation of LC3, emphasizing multiple areas of potential pain from improper form.\"\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"LC5\",\n",
    "        \"timestamp\": \"00:00:26\",\n",
    "        \"quote\": \"let's see how to do it correctly\",\n",
    "        \"clue\": \"This phrase suggests a transition is about to occur. The incorrect form has been shown, and correct form will follow.\"\n",
    "      }\n",
    "    ]\n",
    "    '''\n",
    "    id: str = Field(description='LC1,LC2...')\n",
    "    timestamp: str = Field(description='the timestamp that is most probable for the clue')\n",
    "    quote: str = Field(description='the quote from the transcript that was used to create this clue')\n",
    "    clue: str = Field(description='the main clue data')\n",
    "    \n",
    "class GlobalClue(BaseModel):\n",
    "    '''\n",
    "    Good global clues examples: [\n",
    "      {\n",
    "        \"id\": \"GC1\",\n",
    "        \"timestamp\": \"00:01:15\",\n",
    "        \"quote\": \"Before we dive into specific techniques, let's talk about safety.\",\n",
    "        \"clue\": \"Introduces the theme of safety in squatting.\",\n",
    "        \"relevance_to_segment\": \"This earlier emphasis on safety provides context for why proper depth is important and why it's being addressed in our segment. It connects to the fear of knee pain mentioned in LC3.\"\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"GC2\",\n",
    "        \"timestamp\": \"00:02:30\",\n",
    "        \"quote\": \"Squatting is a fundamental movement pattern in everyday life.\",\n",
    "        \"clue\": \"Emphasizes the importance of squats beyond just exercise.\",\n",
    "        \"relevance_to_segment\": \"This broader context heightens the importance of learning proper squat depth as demonstrated in our segment. It suggests that the techniques shown have applications beyond just gym workouts.\"\n",
    "      },\n",
    "      {\n",
    "        \"clue_id\": \"GC3\",\n",
    "        \"timestamp\": \"00:05:20\",\n",
    "        \"quote\": \"If you have existing knee issues, consult a physician before attempting deep squats.\",\n",
    "        \"clue\": \"Provides a health disclaimer related to squat depth.\",\n",
    "        \"relevance_to_segment\": \"While this comes after our segment, it's relevant because it addresses the concern about knee pain mentioned in LC3. It suggests that the demonstration in our segment is generally safe but acknowledges individual variations.\"\n",
    "      },\n",
    "      {\n",
    "        \"clue_id\": \"GC4\",\n",
    "        \"timestamp\": \"00:06:45\",\n",
    "        \"quote\": \"Proper depth ensures full engagement of your quadriceps and glutes.\",\n",
    "        \"clue\": \"Explains the benefit of correct squat depth.\",\n",
    "        \"relevance_to_segment\": \"This later explanation provides justification for the depth guideline given in LC4. It helps viewers understand why the demonstrated technique is important.\"\n",
    "      },\n",
    "      {\n",
    "        \"clue_id\": \"GC5\",\n",
    "        \"timestamp\": \"00:00:30\",\n",
    "        \"quote\": \"Today, we'll cover squat variations for beginners to advanced lifters.\",\n",
    "        \"clue\": \"Outlines the scope of the entire video.\",\n",
    "        \"relevance_to_segment\": \"This early statement suggests that our segment, focusing on proper depth, is part of a comprehensive guide. It implies that the demonstration might be adaptable for different skill levels.\"\n",
    "      }\n",
    "    ]\n",
    "    '''\n",
    "    id: str = Field(description='GC1,GC2...')\n",
    "    timestamp: str = Field(description='the timestamp that is most probable for the clue')\n",
    "    quote: str = Field(description='the quote from the transcript that was used to create this clue')\n",
    "    clue: str = Field(description='the main clue data')\n",
    "    relevance_to_segment: str = Field(description='why do you think this global clue is relevant to the *part* you are working with right now')\n",
    "\n",
    "class AdditionalInformation(BaseModel):\n",
    "    '''\n",
    "    Good logical inference examples:\n",
    "    [\n",
    "      {\n",
    "        \"id\": \"LI1\",\n",
    "        \"description\": \"Primary Demonstration of Heel Lift\",\n",
    "        \"details\": \"Given that GC1-GC3 describe the 'most common mistake' as heels lifting off the ground, and this description immediately precedes our segment, it's highly probable that this is the primary error being demonstrated. This is further supported by the segment's focus on incorrect form (LC1-LC4).\"\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"LI2\",\n",
    "        \"description\": \"Multiple Error Demonstration\",\n",
    "        \"details\": \"While heel lift is likely the primary focus, the mention of multiple pain points (knees, lower back, shoulders in LC3-LC4) suggests that the demonstrator may be exhibiting several forms of incorrect technique simultaneously. This comprehensive 'what not to do' approach would be pedagogically effective.\"\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"LI3\",\n",
    "        \"description\": \"Possible Inclusion of 'Butt Wink'\",\n",
    "        \"details\": \"Although 'butt wink' is mentioned after our segment (GC4-GC6), its connection to back pain (which is mentioned in LC4) raises the possibility that this error is also present in the demonstration. The instructor may be showing multiple errors early on, then breaking them down individually later.\"\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"LI4\",\n",
    "        \"description\": \"Segment Placement in Overall Video Structure\",\n",
    "        \"details\": \"The segment's position (starting at 00:00:19) and the phrase 'let's see how to do it correctly' (LC5) at the end suggest this is an early, foundational part of the video. It likely serves to grab attention by showing common mistakes before transitioning to proper form instruction.\"\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"LI5\",\n",
    "        \"description\": \"Intentional Exaggeration of Errors\",\n",
    "        \"details\": \"Given the educational nature of the video, it's plausible that the demonstrator is intentionally exaggerating the incorrect form. This would make the errors more obvious to viewers and enhance the contrast with correct form shown later.\"\n",
    "      }\n",
    "    ]\n",
    "    '''\n",
    "    id: str = Field(description='LI1,LI2,...')\n",
    "    description: str = Field(description='A concise form of the logical inference')\n",
    "    details: str = Field(description='A verbose explanation of what insight about what happens in this *part* should be made based on delivered clues')\n",
    "\n",
    "class SegmentAnnotation(BaseModel):\n",
    "    local_clues: list[LocalClue] = Field(description='''Local clues are positioned very close to the *part* of the video in \n",
    "                                              terms of timestamps.''')\n",
    "    global_clues: list[GlobalClue] = Field(description='''Global clues are scattered across the entire transcript. Be very carefull with\n",
    "                                              them as it's very easy to accidentally assume wrong global clue because of limited attention or IQ. \n",
    "                                              ''')\n",
    "    logical_inferences: list[AdditionalInformation] = Field(description='''What guess about how a person performs squats in this *part* can we make based on clues''')\n",
    "\n",
    "# we will only take the segments where the \"doing_squats\" field is positive.\n",
    "clues = generate_clues(\n",
    "    config=config,\n",
    "    annotation_schema=SegmentAnnotation,\n",
    "    human_prompt=human_prompt,\n",
    "    segments_per_call=5,\n",
    "    raise_on_error=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HFnSsLIB7a4 - started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:41<02:03, 41.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 validation errors for VideoAnnotation\n",
      "segments -> 0 -> segment_annotation\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "segments -> 1 -> segment_annotation\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "segments -> 2 -> segment_annotation\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "segments -> 3 -> segment_annotation\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "segments -> 4 -> segment_annotation\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "segments -> 5 -> segment_annotation\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "segments -> 6 -> segment_annotation\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "segments -> 7 -> segment_annotation\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "segments -> 8 -> segment_annotation\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "segments -> 9 -> segment_annotation\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "segments -> 10 -> segment_annotation\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "segments -> 11 -> segment_annotation\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "segments -> 12 -> segment_annotation\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "segments -> 13 -> segment_annotation\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "segments -> 14 -> segment_annotation\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "segments -> 15 -> segment_annotation\n",
      "  value is not a valid dict (type=type_error.dict)\n",
      "Error while generating annotations for HFnSsLIB7a4, skipping\n",
      "gcNh17Ckjgg - started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:57<00:53, 26.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcNh17Ckjgg - done\n",
      "EbOPpWi4L8s - started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [01:07<00:19, 19.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EbOPpWi4L8s - done\n",
      "xqvCmoLULNY - started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:17<00:00, 19.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xqvCmoLULNY - done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datagen.annotate import generate_annotations, generate_clues\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "# This information that will be extracted for each segment from the transcript and data from the previous step.\n",
    "# This is the most important part for the annotation, and getting good results requires a lot of experimenting.\n",
    "\n",
    "import inspect\n",
    "\n",
    "human_prompt = '''\n",
    "You are given a JSON object that contains clues about segments of a video with timecodes.\n",
    "!!!! For each segment provided in a JSON object you need to answer on the following questions:\n",
    "1. Given the data found in the JSON object, what is a probability that this part contains a footage of a person doing squats? [the answer could be only \"High\",\"Medium\" or \"Low\"]\n",
    "2. Given the data found in the JSON object and even if the answer on the previous question is \"Low\", does this person do squats right, wrong, or mixed? [the answer could be only \"Right\", \"Wrong\", and \"Mixed\"]\n",
    "3. Given the data found in the JSON object, what exactly does thing person do right and/or wrong regarding their squats technique? [the answer should be clear and focused on body parts]\n",
    "4. If the answer on the previous question contains description of wrong technique, explain how to fix these mistakes using your \"own knowledge\" like you are a sports coach.\n",
    "'''\n",
    "\n",
    "class SegmentFeedback(BaseModel):\n",
    "    '''\n",
    "—> GOOD EXAMPLES:\n",
    "    \"wrong\":\"Knees caving in: This can stress the knees and reduce effectiveness\"\n",
    "    \"correction\":\"Focus on keeping knees aligned with your toes.\"\n",
    "    \"wrong\":\"Rounding the back: This increases the risk of back injuries\"\n",
    "    \"correction\":\"Keep your chest up and maintain a neutral spine throughout the movement.\"\n",
    "    \"wrong\":\"Heels are lifting off the ground: this shifts the weight forward, reducing stability\"\n",
    "    \"correction\":\" Keep your weight on your heels and press through them as you rise.\"\n",
    "    \"right\":\"Chest and shoulders: The chest is up, and the shoulders are back, maintaining an upright torso.\"\n",
    "    \"correction\":null\n",
    "—> BAD EXAMPLES:\n",
    "    \"wrong\":\"knees\"\n",
    "    \"correction\":\"fix knees\"\n",
    "    \"wrong\":\"back looks funny\"\n",
    "    \"correction\":\"make back better\"\n",
    "    \"wrong\":\"feet are doing something\"\n",
    "    \"correction\":\"feet should be different\"\n",
    "    \"right\":\"arms\"\n",
    "    \"correction\":\"arms are fine i think\"\n",
    "—> BAD EXAMPLES END HERE\n",
    "    '''\n",
    "    right: Optional[str] = Field(description='what was right in the performance')\n",
    "    wrong: Optional[str] = Field(description='what was wrong in the performance')\n",
    "    correction: Optional[str] = Field(description='how and in what ways it the performance could be improved')\n",
    "\n",
    "# The segment timestamps are taken from the provided information.\n",
    "class SegmentAnnotation(BaseModel):\n",
    "    '''\n",
    "Here is a JSON object that contains data about parts with timecodes of a video file that's called \"How to do squats: rights and wrongs\".\n",
    "                !!!! Answer on the following questions:\n",
    "                1. Given the data found in the JSON object, what is a propability that this part contains a footage of a person doing squats? [the answer could be only \"High\",\"Medium\" or \"Low\"]\n",
    "                2. Given the data found in the JSON object and even if the answer on the previous question is \"Low\", does this person do squats right, wrong, or mixed? [the answer could be only \"Right\", \"Wrong\", and \"Mixed\"]\n",
    "                3. Given the data found in the JSON object, what exactly does thing person do right and/or wrong regarding their squats technique? [the answer should be clear and focused on body parts]\n",
    "                4. If the answer on the previous question contains description of wrong technique, explain how to fix these mistakes using your \"own knowledge\" like you are a sports coach.\n",
    "\n",
    "    '''\n",
    "    squats_probability: Optional[str] = Field(description='how high is the probability that the person is doing squats in the segment: low, medium, high, unknown(null)')\n",
    "    squats_technique_correctness: Optional[bool] = Field(description='bollean correctness of the squat technique.')\n",
    "    squats_feedback: SegmentFeedback = Field(description='what was right and wrong in the squat perfomance in the segment. When the technique is incorrect, provide instructions how to correct them.')\n",
    "\n",
    "# we will only take the segments where the \"doing_squats\" field is positive.\n",
    "annotations = generate_annotations(\n",
    "    # human_prompt=human_prompt,\n",
    "    config=config,\n",
    "    annotation_schema=SegmentAnnotation,\n",
    "    # filter_by='doing_squats'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get a list of annotations for each video:\n",
    "```\n",
    "[\n",
    "    {\n",
    "        \"start_timestamp\": \"00:00:51.760\",\n",
    "        \"end_timestamp\": \"00:01:01.520\",\n",
    "        \"segment_annotation\": {\n",
    "            \"correct\": null,\n",
    "            \"incorrect_reasons\": null,\n",
    "            \"qa\": [\n",
    "                {\n",
    "                    \"question\": \"Was there important advice about performing the exercise correctly?\",\n",
    "                    \"answer\": \"Yes, the advice was to make sure the knees do not go forward of the toes.\",\n",
    "                    \"quote\": \"making sure that your knees do not go forward of your toes\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping gcNh17Ckjgg\n",
      "Total segments: 22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'start_timestamp': '00:00:20.479',\n",
       " 'end_timestamp': '00:00:26.485',\n",
       " 'segment_annotation': {'correct': None,\n",
       "  'incorrect_reasons': None,\n",
       "  'qa': [{'question': 'Was the exercise (squat) performed correctly?',\n",
       "    'answer': 'Yes, the squat exercise was described correctly.',\n",
       "    'quote': \"let's learn how to properly perform a squat...cross your arms in front...shift your weight to the ball of your feet...bend your knees...push back up to the starting position.\"}]},\n",
       " 'video_id': 'xqvCmoLULNY',\n",
       " 'id': 'xqvCmoLULNY_0',\n",
       " 'video_path': 'xqvCmoLULNY_0.mp4'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datagen import aggregate_annotations\n",
    "\n",
    "# saved to annotations.json\n",
    "annotations = aggregate_annotations(config)\n",
    "print('Total segments:', len(annotations))\n",
    "annotations[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The last step is to cut video clips for annotated segments from original videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:14<00:00,  1.55it/s]\n"
     ]
    }
   ],
   "source": [
    "from datagen import cut_videos\n",
    "cut_videos(config=config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as a result we generated:\n",
    "- `<data_dir>/clips/` with video clips that you can use for training\n",
    "- `<data_dir>/annotations.json` with list of items with fields:\n",
    "    - video_id: 11-char youtube video id (youtube.com/watch?v=<id>)\n",
    "    - start_timestamp/end_timestamp of the clip relative to the youtube video it's taken from\n",
    "    - video_path of the clip relative to `<data_dir>/clips/`\n",
    "    - segment_annotation that you can use for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
