{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Data Generation SDK\n",
    "\n",
    "We are going to generate a dataset of squat videos with instructions how to perform them, so that we can train an AI pesonal trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from datagen import DatagenConfig\n",
    "\n",
    "config_params = {\n",
    "    'openai': {\n",
    "        'type': 'azure', # openai/azure\n",
    "        'temperature': '1',\n",
    "        'deployment': 'gpt4o' # model for openai / deployment for azure\n",
    "    },\n",
    "    'data_dir': './tmp/squats'\n",
    "}\n",
    "\n",
    "# this config handles all the bookeeping so you need to pass it everywhere.\n",
    "config = DatagenConfig(**config_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a list of search queries to search for videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how to do squats',\n",
       " 'squat exercise tutorial',\n",
       " 'beginner guide to squats',\n",
       " 'proper squat form',\n",
       " 'squat workout video']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datagen import get_queries\n",
    "queries = get_queries(\n",
    "    config=config,\n",
    "    prompt='I want to find instructional videos about how to do squats.',\n",
    "    num_queries=2\n",
    ")\n",
    "queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download video information for each query.\n",
    "\n",
    "We'll get 2 videos for each query.<br>\n",
    "One video might be found with multiple queries, so we might get less than `n_queries*videos_per_query` videos.<br>\n",
    "If you want to get all youtube videos for a query, don't pass `videos_per_query` parameter.\n",
    "\n",
    "You can limit the search to only videos licensed with Creative Commons (as indicated by youtube).<br>\n",
    "As this search isn't directly implemented in searching libraries yet, we search for all videos and filter for license afterwards.<br>\n",
    "Unfortunately, this way you will likely get very few results, so use with caution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:03<00:00,  1.26it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['KJ8xAMJdZjQ',\n",
       " 'ubdIGnX2Hfs',\n",
       " 'YaXPRqUwItQ',\n",
       " 'l83R5PblSMA',\n",
       " 'irfw1gQ0foQ',\n",
       " 'dCHLUtf--pg',\n",
       " 'PPmvh7gBTi0',\n",
       " 'EbOPpWi4L8s',\n",
       " '4KmY44Xsg2w',\n",
       " '3qkgrJNB6kA',\n",
       " 'IB_icWRzi4E',\n",
       " 'HFnSsLIB7a4',\n",
       " 'xqvCmoLULNY',\n",
       " 'LSj280OEKUI',\n",
       " 'gcNh17Ckjgg',\n",
       " 'p-R0HSfL6nw',\n",
       " 'DGhHgiCfAb0',\n",
       " '_uZLFUnKSaM',\n",
       " 'byxWus7BwfQ',\n",
       " 'xuf1czJv-XI']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datagen import get_video_ids\n",
    "ids = get_video_ids(queries, config=config, videos_per_query=2, only_creative_commons=False)\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download videos and autogenerated subtitles\n",
    "\n",
    "You can change sub languages, formats etc with `yt_dlp_opts` dictionary (refer to https://github.com/yt-dlp/yt-dlp).<br>\n",
    "The SDK is expecting `.mp4` video files (for now), so don't change that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=ubdIGnX2Hfs\n",
      "[youtube] ubdIGnX2Hfs: Downloading webpage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] Webpage contains broken formats (poToken experiment detected). Ignoring initial player response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] ubdIGnX2Hfs: Downloading ios player API JSON\n",
      "[youtube] ubdIGnX2Hfs: Downloading player d2e656ee\n",
      "[youtube] ubdIGnX2Hfs: Downloading web player API JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] ubdIGnX2Hfs: nsig extraction failed: Some formats may be missing\n",
      "         n = 4fJ2aQguR3YtedGA ; player = https://www.youtube.com/s/player/d2e656ee/player_ias.vflset/en_US/base.js\n",
      "WARNING: [youtube] ubdIGnX2Hfs: nsig extraction failed: Some formats may be missing\n",
      "         n = 6nvrBUMEriQ9VtTj ; player = https://www.youtube.com/s/player/d2e656ee/player_ias.vflset/en_US/base.js\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] ubdIGnX2Hfs: Downloading m3u8 information\n",
      "[info] ubdIGnX2Hfs: Downloading subtitles: en\n",
      "[info] ubdIGnX2Hfs: Downloading 1 format(s): 617\n",
      "[info] Writing video subtitles to: tmp/squats3/videos/ubdIGnX2Hfs.en.vtt\n",
      "[download] Destination: tmp/squats3/videos/ubdIGnX2Hfs.en.vtt\n",
      "[download] 100% of   41.03KiB in 00:00:00 at 84.70KiB/s\n",
      "[hlsnative] Downloading m3u8 manifest\n",
      "[hlsnative] Total fragments: 74\n",
      "[download] Destination: tmp/squats3/videos/ubdIGnX2Hfs.mp4\n",
      "[download] 100% of  126.46MiB in 00:00:42 at 3.00MiB/s                  \n",
      "[MoveFiles] Moving file \"tmp/squats3/videos/ubdIGnX2Hfs.en.vtt\" to \"tmp/squats3/subs/ubdIGnX2Hfs.en.vtt\"\n"
     ]
    }
   ],
   "source": [
    "from datagen import download_videos\n",
    "download_videos(ids, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect segments from video\n",
    "\n",
    "We will use the clip version because it's much faster than gpt4o, but we'll need a GPU.\n",
    "You can also try using CPU for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModel\n",
    "\n",
    "# remove .cuda() for cpu\n",
    "# SIGLIP outputs independent probs as opposed to CLIP that outputs multiclass probs\n",
    "model = AutoModel.from_pretrained(\"google/siglip-so400m-patch14-384\").cuda()\n",
    "processor = AutoProcessor.from_pretrained(\"google/siglip-so400m-patch14-384\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HFnSsLIB7a4 - starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1/13 [00:41<08:18, 41.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs (743,) frames 743\n",
      "ubdIGnX2Hfs - starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 2/13 [01:25<07:53, 43.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs (825,) frames 825\n",
      "p-R0HSfL6nw - starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 3/13 [02:39<09:30, 57.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs (1372,) frames 1372\n",
      "byxWus7BwfQ - starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 4/13 [03:00<06:27, 43.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs (393,) frames 393\n",
      "EbOPpWi4L8s - starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 5/13 [03:11<04:10, 31.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs (193,) frames 193\n",
      "KJ8xAMJdZjQ - starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 6/13 [03:18<02:41, 23.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs (133,) frames 133\n",
      "dCHLUtf--pg - starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 7/13 [04:03<03:00, 30.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs (810,) frames 810\n",
      "l83R5PblSMA - starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 8/13 [04:05<01:45, 21.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs (33,) frames 33\n",
      "xuf1czJv-XI - starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 9/13 [04:14<01:09, 17.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs (170,) frames 170\n",
      "LSj280OEKUI - starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 10/13 [05:29<01:46, 35.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs (1410,) frames 1410\n",
      "irfw1gQ0foQ - starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 11/13 [06:41<01:32, 46.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs (1284,) frames 1284\n",
      "3qkgrJNB6kA - starting\n",
      "probs (8513,) frames 8513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 12/13 [14:43<02:58, 178.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DGhHgiCfAb0 - starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [16:05<00:00, 74.25s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs (1521,) frames 1521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datagen import detect_segments_clip\n",
    "\n",
    "from typing import Optional\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "detect_segments_clip(\n",
    "    # video_ids=['KvRK5Owqzgw'],\n",
    "    text_prompts='a person doing squats', # that's the text for CLIP to compare to images. You can provide a list of texts to use the average distance.\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    fps_sampling=2, # the more fps, the more granular segment borders and more precise segments, at the cost of speed.\n",
    "    device='cuda', # 'cpu' for local\n",
    "    frames_per_batch=100, # 100 frames use about 10GB GPU RAM, so batch to fill your GPU RAM.\n",
    "    config=config,\n",
    "\n",
    "    # Parameters for segment detection from probabilities - these default values should work well, but if they produce bad results for specific kinds of videos, you can adjust them.\n",
    "    min_prob=0.1, # minimum CLIP probability to consider the match\n",
    "    max_gap_seconds=1, # gaps of prob < min_prob that could be inside segment\n",
    "    min_segment_seconds=3, # discard very short segments\n",
    "    smooth_fraction=0.02, # smoothing strength. Raw probabilities are smoothed to adapt to fluctuations between frames.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each video we get a list of segments:\n",
    "```\n",
    "[\n",
    "    ...\n",
    "    {\n",
    "        \"start_timestamp\": \"00:00:32.500\",\n",
    "        \"end_timestamp\": \"00:00:41.500\",\n",
    "        \"fps\": 29.97002997002997,\n",
    "        \"segment_info\": null, # not used with clip, but could be used with gpt4o\n",
    "        \"video_id\": \"KvRK5Owqzgw\"\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotaion step 1: extract information (clues) from transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "byxWus7BwfQ - started\n",
      "byxWus7BwfQ part 0 - started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:25<00:00, 25.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "byxWus7BwfQ - done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datagen import generate_clues\n",
    "\n",
    "human_prompt = \"\"\"\n",
    "The provided video is a tutorial about how to perform squats. \n",
    "\n",
    "I need to understand HOW THE PERSON SHOWN IN EACH SEGMENT PERFORMS SQUATS IN THIS SEGMENT.\n",
    "What is done correctly.\n",
    "What mistakes they make. Why these mistakes happen.\n",
    "How these mistakes could be improved.\n",
    "\n",
    "It is very improtant that the information that you provide would describe how the person shown in the segment is doing squats, and not some generic advice that is unrelated to the visual information.\n",
    "\"\"\"\n",
    "\n",
    "clues = generate_clues(\n",
    "    # video_ids=['byxWus7BwfQ'],\n",
    "    config=config,\n",
    "    human_prompt=human_prompt,\n",
    "    segments_per_call=5, # the output might be quite long, so need to limit number of segments per gpt call to respect max output legnth\n",
    "    raise_on_error=True, # interrupt when encountering an error. Useful for debugging.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_clues' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sleep\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 3\u001b[0m     clues \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_clues\u001b[49m(\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;66;03m# video_ids=['byxWus7BwfQ'],\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m      6\u001b[0m         human_prompt\u001b[38;5;241m=\u001b[39mhuman_prompt,\n\u001b[1;32m      7\u001b[0m         segments_per_call\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;66;03m# the output might be quite long, so need to limit number of segments per gpt call to respect max output legnth\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         raise_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;66;03m# interrupt when encountering an error. Useful for debugging.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     )\n\u001b[1;32m     10\u001b[0m     sleep(\u001b[38;5;241m60\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_clues' is not defined"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "while True:\n",
    "    clues = generate_clues(\n",
    "        # video_ids=['byxWus7BwfQ'],\n",
    "        config=config,\n",
    "        human_prompt=human_prompt,\n",
    "        segments_per_call=5, # the output might be quite long, so need to limit number of segments per gpt call to respect max output legnth\n",
    "        raise_on_error=True, # interrupt when encountering an error. Useful for debugging.\n",
    "    )\n",
    "    sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotaion step 2: extract information from transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KJ8xAMJdZjQ - started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KJ8xAMJdZjQ - done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datagen import generate_annotations\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "# This information that will be extracted for each segment from the transcript and data from the previous step.\n",
    "# This is the most important part for the annotation, and getting good results requires a lot of experimenting.\n",
    "\n",
    "\n",
    "human_prompt = '''\n",
    "You are given a JSON object that contains clues about segments of a video with timecodes.\n",
    "!!!! For each segment provided in a JSON object you need to answer on the following questions:\n",
    "1. Given the data found in the JSON object, what is a probability that this part contains a footage of a person doing squats? [the answer could be only \"high\", \"medium\", \"low\", or null (if impossible to infer from the provided data)]\n",
    "2. Given the data found in the JSON object and even if the answer on the previous question is \"low\", does this person do squats right, wrong, or mixed? [the answer could be only \"right\", \"wrong\", \"mixed\", or null (if impossible to infer from the provided data)]\n",
    "3. Given the data found in the JSON object, what exactly does thing person do right and/or wrong regarding their squats technique? [the answer should be clear and focused on body parts]\n",
    "4. If the answer on the previous question contains description of wrong technique, explain how to fix these mistakes using your \"own knowledge\" like you are a sports coach.\n",
    "'''\n",
    "\n",
    "class SegmentFeedback(BaseModel):\n",
    "    '''\n",
    "—> GOOD EXAMPLES:\n",
    "    \"wrong\":\"Knees caving in: This can stress the knees and reduce effectiveness\"\n",
    "    \"correction\":\"Focus on keeping knees aligned with your toes.\"\n",
    "    \"wrong\":\"Rounding the back: This increases the risk of back injuries\"\n",
    "    \"correction\":\"Keep your chest up and maintain a neutral spine throughout the movement.\"\n",
    "    \"wrong\":\"Heels are lifting off the ground: this shifts the weight forward, reducing stability\"\n",
    "    \"correction\":\" Keep your weight on your heels and press through them as you rise.\"\n",
    "    \"right\":\"Chest and shoulders: The chest is up, and the shoulders are back, maintaining an upright torso.\"\n",
    "    \"correction\":null\n",
    "—> BAD EXAMPLES:\n",
    "    \"wrong\":\"knees\"\n",
    "    \"correction\":\"fix knees\"\n",
    "    \"wrong\":\"back looks funny\"\n",
    "    \"correction\":\"make back better\"\n",
    "    \"wrong\":\"feet are doing something\"\n",
    "    \"correction\":\"feet should be different\"\n",
    "    \"right\":\"arms\"\n",
    "    \"correction\":\"arms are fine i think\"\n",
    "—> BAD EXAMPLES END HERE\n",
    "    '''\n",
    "    right: Optional[str] = Field(description='what was right in the performance')\n",
    "    wrong: Optional[str] = Field(description='what was wrong in the performance')\n",
    "    correction: Optional[str] = Field(description='how and in what ways it the performance could be improved')\n",
    "\n",
    "# The segment timestamps are taken from the provided information.\n",
    "class SegmentAnnotation(BaseModel):\n",
    "    '''\n",
    "Here is a JSON object that contains data about parts with timecodes of a video file where a person does squats.\n",
    "!!!! Answer on the following questions:\n",
    "1. Given the data found in the JSON object, what is a propability that this part contains a footage of a person doing squats? [the answer could be only \"high\",\"medium\" or \"low\"]\n",
    "2. Given the data found in the JSON object and even if the answer on the previous question is \"low\", does this person do squats correctly or not?\n",
    "3. Given the data found in the JSON object, what exactly does thing person do right and/or wrong regarding their squats technique? [the answer should be clear and focused on body parts]\n",
    "4. If the answer on the previous question contains description of wrong technique, explain how to fix these mistakes using your \"own knowledge\" like you are a sports coach.\n",
    "    '''\n",
    "    squats_probability: Optional[str] = Field(description='how high is the probability that the person is doing squats in the segment: low, medium, high, unknown(null)')\n",
    "    squats_technique_correctness: Optional[str] = Field(description='correctness of the squat technique.')\n",
    "    squats_feedback: Optional[SegmentFeedback] = Field(description='what was right and wrong in the squat perfomance in the segment. When the technique is incorrect, provide instructions how to correct them.')\n",
    "\n",
    "# we will only take the segments where the \"doing_squats\" field is positive.\n",
    "annotations = generate_annotations(\n",
    "    human_prompt=human_prompt,\n",
    "    config=config,\n",
    "    segments_per_call=5,\n",
    "    annotation_schema=SegmentAnnotation,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get a list of annotations for each video:\n",
    "```\n",
    "[\n",
    "    {\n",
    "        \"start_timestamp\": \"00:00:51.760\",\n",
    "        \"end_timestamp\": \"00:01:01.520\",\n",
    "        \"segment_annotation\": {\n",
    "            \"correct\": null,\n",
    "            \"incorrect_reasons\": null,\n",
    "            \"qa\": [\n",
    "                {\n",
    "                    \"question\": \"Was there important advice about performing the exercise correctly?\",\n",
    "                    \"answer\": \"Yes, the advice was to make sure the knees do not go forward of the toes.\",\n",
    "                    \"quote\": \"making sure that your knees do not go forward of your toes\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 88208.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total segments: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'start_timestamp': '00:01:20.250',\n",
       " 'end_timestamp': '00:01:25.250',\n",
       " 'segment_annotation': {'squats_probability': 'medium',\n",
       "  'squats_technique_correctness': 'mixed',\n",
       "  'squats_feedback': {'right': 'Correct knee alignment: Not letting the knees go past a specific line.',\n",
       "   'wrong': 'Potential common mistakes as seen in weight rooms and competitions.',\n",
       "   'correction': 'Focus on proper knee alignment to prevent potential damage: Ensure your knees do not pass a plumb line drawn from your toes during the squat.'}},\n",
       " 'video_id': 'byxWus7BwfQ',\n",
       " 'id': 'byxWus7BwfQ_0',\n",
       " 'video_path': 'byxWus7BwfQ_0.mp4'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datagen import aggregate_annotations\n",
    "\n",
    "# saved to annotations.json\n",
    "\n",
    "def filter_annotations(ann):\n",
    "    if ann['squats_probability'] in [None, 'low', 'None', 'null']:\n",
    "        # if we're not able to infer probability or prob is low, we don't need it\n",
    "        return False\n",
    "    if ann['squats_technique_correctness'] in [None, 'null', 'None']:\n",
    "        # if we couldnt establish correctness at all, the feedback is probably useless\n",
    "        return False\n",
    "    if ann['squats_technique_correctness'] in ['mixed']:\n",
    "        # discard empty segment if correctness isn't clear since there isn't any information to use for training\n",
    "        if ann['squats_feedback'] is None:\n",
    "            return False\n",
    "        if set(ann['squats_feedback'].values()) == set([None]):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "annotations = aggregate_annotations(config, filter_func=filter_annotations, annotation_file='annotations.json')\n",
    "print('Total segments:', len(annotations))\n",
    "annotations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The last step is to cut video clips for annotated segments from original videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:14<00:00,  1.55it/s]\n"
     ]
    }
   ],
   "source": [
    "from datagen import cut_videos\n",
    "cut_videos(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as a result we generated:\n",
    "- `<data_dir>/clips/` with video clips that you can use for training\n",
    "- `<data_dir>/annotations.json` with list of items with fields:\n",
    "    - video_id: 11-char youtube video id (youtube.com/watch?v=<id>)\n",
    "    - start_timestamp/end_timestamp of the clip relative to the youtube video it's taken from\n",
    "    - video_path of the clip relative to `<data_dir>/clips/`\n",
    "    - segment_annotation that you can use for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
